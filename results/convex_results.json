[
  {
    "statement": "Fix any L>0 and any relative noise level \\(\\delta\\in(0,1)\\). There does not exist a positive universal step size \\(\\eta_{\\max}(\\delta,L)>0\\) with the property that for every (even one‑dimensional) convex L‑smooth quadratic f, every nonzero initialization x_0\\ne0, and every sequence of inexact‑gradient noises satisfying \\(\\|e_n\\|\\le\\delta\\|\\nabla f(x_n)\\|\\), the inexact gradient descent iterates \\(x_{n+1}=x_n-\\eta(\\nabla f(x_n)+e_n)\\) produce a convex sequence of function values \\(n\\mapsto f(x_n)\\) for every choice of step size \\(0<\\eta\\le\\eta_{\\max}(\\delta,L)\\). In particular, no nonzero universal convexity‑preserving step‑size depending only on \\(\\delta\\) and \\(L\\) (and valid for all one‑dimensional convex L‑smooth quadratics and all nonzero initializations) exists when \\(\\delta>0\\).",
    "proof_markdown": "Assume, for contradiction, that there exist fixed constants \\(L>0\\) and \\(\\delta\\in(0,1)\\) and a positive universal step size \\(\\eta_{\\max}(\\delta,L)>0\\) with the stated property: for every one‑dimensional convex L‑smooth quadratic f, every nonzero initialization, and every noise sequence satisfying \\(\\|e_n\\|\\le\\delta\\|\\nabla f(x_n)\\|\\), the energy sequence \\(n\\mapsto f(x_n)\\) produced by inexact GD with any \\(0<\\eta\\le\\eta_{\\max}(\\delta,L)\\) is convex. Fix any \\(0<\\eta<\\eta_{\\max}(\\delta,L)\\); we will construct a one‑dimensional convex L‑smooth quadratic f and an admissible noise sequence that makes the energy sequence nonconvex, contradicting the assumed universality of \\(\\eta_{\\max}\\).\n\nTake the quadratic\n\n$$\nf(x)=\\tfrac{L}{2}x^2,\n$$\n\nwhich is convex and L‑smooth with gradient \\(\\nabla f(x)=Lx\\). Parametrize admissible multiplicative noise by writing\n\n$$\ne_n=\\varepsilon_n L x_n,\n$$\n\nwith \\(|\\varepsilon_n|\\le\\delta\\), so the inexact GD update becomes\n\n$$\nx_{n+1}=x_n-\\eta\\bigl(Lx_n+e_n\\bigr)=x_n-\\eta L x_n(1+\\varepsilon_n).\n$$\n\nFix any nonzero initial point \\(x_0\\ne0\\). Use the following admissible two‑step adversarial pattern for the multiplicative noise:\n\n$$\n\\varepsilon_0=-\\delta,\\qquad \\varepsilon_1=+\\delta,\n$$\n\n(and arbitrary \\(\\varepsilon_n\\in[-\\delta,\\delta]\\) thereafter; only the first two steps are used). Put\n\n$$\n\\alpha:=\\eta L,\\qquad \\theta_0:=1-\\delta,\\qquad \\theta_1:=1+\\delta.\n$$\n\nThen\n\n$$\nx_1=x_0(1-\\alpha\\theta_0),\\qquad x_2=x_1(1-\\alpha\\theta_1)=x_0(1-\\alpha\\theta_0)(1-\\alpha\\theta_1).\n$$\n\nThe function values are \\(f(x_n)=\\tfrac{L}{2}x_n^2\\). Define the forward differences\n\n$$\n\\Delta_n:=f(x_n)-f(x_{n+1}).\n$$\n\nA direct calculation yields\n\n\\begin{align*}\n\\Delta_0 &= \\tfrac{L}{2}\\bigl(x_0^2-x_1^2\\bigr)=Lx_0^2\\bigl(\\alpha\\theta_0 - \\tfrac{\\alpha^2\\theta_0^2}{2}\\bigr),\\\\\n\\Delta_1 &= \\tfrac{L}{2}\\bigl(x_1^2-x_2^2\\bigr)=Lx_0^2(1-\\alpha\\theta_0)^2\\bigl(\\alpha\\theta_1-\\tfrac{\\alpha^2\\theta_1^2}{2}\\bigr).\n\\end{align*}\n\nHence\n\n$$\n\\Delta_0-\\Delta_1 = Lx_0^2\\,\\alpha\\,S(\\alpha),\n$$\n\nwhere\n\n$$\nS(\\alpha)=\\theta_0\\Bigl(1-\\tfrac{\\alpha\\theta_0}{2}\\Bigr)-\\theta_1(1-\\alpha\\theta_0)^2\\Bigl(1-\\tfrac{\\alpha\\theta_1}{2}\\Bigr).\n$$\n\nEvaluate at \\(\\alpha=0\\):\n\n$$\nS(0)=\\theta_0-\\theta_1=(1-\\delta)-(1+\\delta)=-2\\delta<0.\n$$\n\nSince \\(S\\) is continuous in \\(\\alpha\\), there exists \\(\\alpha^*>0\\) such that \\(S(\\alpha)<0\\) for every \\(\\alpha\\in(0,\\alpha^*)\\). Choosing any \\(\\eta\\) with \\(0<\\eta<\\min\\{\\eta_{\\max}(\\delta,L),\\alpha^*/L\\}\\) gives \\(\\alpha=\\eta L\\in(0,\\alpha^*)\\) and hence\n\n$$\n\\Delta_0-\\Delta_1 = Lx_0^2\\,\\alpha\\,S(\\alpha) < 0,\n$$\n\ni.e. \\(\\Delta_0<\\Delta_1\\). Therefore the forward differences are not nonincreasing already at the first pair \\(\\{\\Delta_0,\\Delta_1\\}\\); equivalently the discrete energy sequence \\(n\\mapsto f(x_n)\\) is not convex for this admissible noise sequence.\n\nThis contradicts the assumed universality of \\(\\eta_{\\max}(\\delta,L)\\). Hence no such positive universal \\(\\eta_{\\max}(\\delta,L)\\) exists for any \\(\\delta\\in(0,1)\\), even when the class of admissible functions is restricted to the one‑dimensional quadratic \\(f(x)=\\tfrac{L}{2}x^2\\) and initializations are nonzero."
  },
  {
    "statement": "Suppose f: \\mathbb{R}^d\\to\\mathbb{R} is convex and twice continuously differentiable on the sublevel set S:=\\{x:\\;f(x)\\le f(x_0)\\} for some initial point x_0. Assume there exist a scalar \\kappa>0 and a symmetric positive semidefinite matrix A with largest eigenvalue L_A such that for every x\\in S one has\n\\[\\nabla^2 f(x)\\preceq \\kappa A.\\]\nSet L_{\\mathrm{eff}}:=\\kappa L_A. Then gradient descent with constant step size\n\\[\\n\\eta\\in\\bigl(0,\\,1.75/ L_{\\mathrm{eff}}\\bigr]\\]\nstarted at x_0 produces a convex optimization curve n\\mapsto f(x_n); equivalently the forward differences \\Delta_n:=f(x_n)-f(x_{n+1}) form a nonincreasing sequence. In particular, if \\kappa=1 and L_A=L then convexity holds for every \\eta\\in(0,\\,1.75/L].",
    "proof_markdown": "**Proof.** Put L_{\\mathrm{eff}}:=\\kappa L_A and let the GD iterates \\{x_n\\}_{n\\ge0} be generated from x_0 with a constant step size\n\\[\\n\\eta\\in\\bigl(0,\\,1.75/L_{\\mathrm{eff}}\\bigr].\n\\]\nWe prove two facts: (1) the GD trajectory remains in S (forward invariance), and (2) the admissible discrete convexity lemma then yields convexity of the optimization curve.\n\nStep 1: forward invariance. Since 1.75/L_{\\mathrm{eff}}<2/L_{\\mathrm{eff}}, it suffices to show forward invariance for any step size in (0,2/L_{\\mathrm{eff}}); the chosen range is a subset of that. We show by induction that x_n\\in S for all n. The base case x_0\\in S is immediate. Suppose x_n\\in S. If \\nabla f(x_n)=0 then x_{n+1}=x_n\\in S. Otherwise set\n\\[\\ng:=\\nabla f(x_n)\\ne0,\\qquad r(t):=x_n - t g,\\qquad \\phi(t):=f(r(t)),\\quad t\\ge0,\n\\]\nso r(\\eta)=x_{n+1} and \\phi(0)=f(x_n)\\le f(x_0). Assume for contradiction that x_{n+1}\\notin S, i.e. \\phi(\\eta)>f(x_0). By continuity there exists a minimal t_*\\in(0,\\eta] with \\phi(t_*)=f(x_0). By minimality r(t)\\in S for all t\\in[0,t_*], so the Hessian bound holds along the segment and, for 0\\le t\\le t_*,\n\\[\\n\\phi'(0)=-\\|g\\|^2,\\qquad \\phi''(t)=g^T\\nabla^2 f(r(t))g\\le g^T(\\kappa A)g\\le \\kappa L_A\\|g\\|^2 = L_{\\mathrm{eff}}\\|g\\|^2.\n\\]\nIntegrating twice the bound on \\phi'' gives for every t\\in[0,t_*]\n\\[\\n\\phi(t)-\\phi(0)\\le -t\\|g\\|^2 + \\tfrac{L_{\\mathrm{eff}}}{2}t^2\\|g\\|^2 = -t\\Bigl(1-\\tfrac{L_{\\mathrm{eff}}t}{2}\\Bigr)\\|g\\|^2.\n\\]\nTaking t=t_* and using \\phi(t_*)-\\phi(0)=f(x_0)-f(x_n)\\ge0 yields\n\\[\\n0\\le -t_*\\Bigl(1-\\tfrac{L_{\\mathrm{eff}}t_*}{2}\\Bigr)\\|g\\|^2,\n\\]\nso 1-\\tfrac{L_{\\mathrm{eff}}t_*}{2}\\le0 and hence t_*\\ge 2/L_{\\mathrm{eff}}. This contradicts t_*\\le\\eta<2/L_{\\mathrm{eff}}, so the assumption x_{n+1}\\notin S is impossible. Therefore x_{n+1}\\in S, and by induction all iterates remain in S.\n\nStep 2: discrete convexity. Because every iterate lies in the convex set S, each segment [x_n,x_{n+1}] is contained in S, and the uniform upper bound on the Hessian on S implies that f|_S is L_{\\mathrm{eff}}-smooth on the convex domain S. By the admissible discrete convexity lemma (which states that for any convex function that is L-smooth on a convex domain containing the GD trajectory, gradient descent with step size \\eta\\in(0,1.75/L] produces a convex optimization curve), applying that lemma to f|_S with L=L_{\\mathrm{eff}} yields that the sequence n\\mapsto f(x_n) is convex for every\n\\[\\n\\eta\\in\\bigl(0,\\,1.75/ L_{\\mathrm{eff}}\\bigr] = \\bigl(0,\\,1.75/(\\kappa L_A)\\bigr].\n\\]\n\nRemark on optimality. The stronger threshold 2/L_{\\mathrm{eff}} cannot be guaranteed uniformly under these hypotheses: there are known convex L-smooth counterexamples where the GD optimization curve fails to be convex for every \\eta\\in(1.75/L,2/L). Taking A=LI and \\kappa=1 places those counterexamples inside the present class, so the constant 1.75 (hence the bound 1.75/(\\kappa L_A)) is the best uniform guarantee under the stated assumptions.\n\n∎"
  },
  {
    "statement": "Let $f(x)=\\tfrac12 x^\\top Qx$ with $Q\\succeq0$. For any $x_0$ and any constant stepsize $\\eta\\ge0$ such that $\\eta\\lambda_i\\in[0,2]$ for every eigenvalue $\\lambda_i$ of $Q$ (in particular, any $\\eta\\in[0,2/L]$ when $L:=\\lambda_{\\max}(Q)>0$), the gradient-descent values $n\\mapsto f(x_n)$ are nonincreasing and convex in $n$; equivalently, the forward differences $\\Delta_n:=f(x_n)-f(x_{n+1})$ satisfy $\\Delta_n\\ge0$ and $\\Delta_{n+1}\\le\\Delta_n$ for all $n\\ge0$.",
    "proof_markdown": "Proof. Let $f(x)=\\tfrac12 x^\\top Qx$ with $Q\\succeq0$, and run gradient descent with constant stepsize $\\eta\\ge0$:\n\\[ x_{n+1}=x_n-\\eta Qx_n=(I-\\eta Q)x_n,\\qquad x_n=(I-\\eta Q)^n x_0. \\]\nTake an orthogonal diagonalization $Q=U\\Lambda U^\\top$ with $\\Lambda=\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d)$ and $\\lambda_i\\ge0$. In the coordinates $y_n:=U^\\top x_n$ we have $y_{n+1}=(I-\\eta\\Lambda)y_n$, hence $y_{n,i}=(1-\\eta\\lambda_i)^n y_{0,i}$. Therefore\n\\[\n f(x_n)=\\tfrac12 y_n^\\top\\Lambda y_n=\\tfrac12\\sum_{i=1}^d \\lambda_i\\,(1-\\eta\\lambda_i)^{2n}\\,y_{0,i}^2.\n\\]\nSet $\\Delta_n:=f(x_n)-f(x_{n+1})$. A direct computation gives\n\\[\n\\Delta_n=\\tfrac12\\sum_{i=1}^d \\lambda_i\\Bigl[1-(1-\\eta\\lambda_i)^2\\Bigr](1-\\eta\\lambda_i)^{2n} y_{0,i}^2.\n\\]\nDefine $s_i:=(1-\\eta\\lambda_i)^2$ and $\\gamma_i:=\\lambda_i\\bigl(1-s_i\\bigr)y_{0,i}^2=\\eta\\lambda_i^{2}(2-\\eta\\lambda_i) y_{0,i}^2\\ge0$. Under the stepsize condition $\\eta\\lambda_i\\in[0,2]$ for all $i$, we have $s_i\\in[0,1]$. Thus\n\\[\n\\Delta_n=\\tfrac12\\sum_{i=1}^d \\gamma_i\\, s_i^{\\,n}\\ge0,\\qquad \\Delta_{n+1}-\\Delta_n=\\tfrac12\\sum_{i=1}^d \\gamma_i\\,s_i^{\\,n}(s_i-1)\\le0.\n\\]\nHence $\\{\\Delta_n\\}_{n\\ge0}$ is nonnegative and nonincreasing. Equivalently, $n\\mapsto f(x_n)$ is nonincreasing and convex. The boundary $\\eta\\lambda_i=2$ is included since $s_i\\in[0,1]$ and $\\gamma_i\\ge0$ still hold (when $\\lambda_i=0$ or $\\eta=0$, one has $\\gamma_i=0$ and the conclusions remain true). ∎"
  },
  {
    "statement": "Stronger counterexample on a whole interval of admissible parameters. There exists a one-dimensional quadratic function f(x)=\\tfrac{L}{2}x^2 (which is L-smooth and L-strongly convex) such that for every step size \\eta\\in[2/(3L),\\,1/L) and momentum parameter \\theta:=1/L-\\eta (so that \\eta\\in(0,2/L] and \\theta\\in[0,\\eta/2]), and for every initial point x_0\\neq0 with x_{-1}=x_0, the iteration\n\\[\n x_{n+1}=x_n-\\eta\\,\\nabla f(x_n)-\\theta\\bigl(\\nabla f(x_n)-\\nabla f(x_{n-1})\\bigr)\n\\]\nproduces a sequence (f(x_n)) whose piecewise-linear interpolation in n is not convex.",
    "proof_markdown": "Take the one-dimensional quadratic f(x)=\\tfrac{L}{2}x^2, so \\nabla f(x)=Lx; this f is L-smooth and L-strongly convex. Fix any \\eta\\in[2/(3L),1/L) and set \\theta:=1/L-\\eta. Then \\eta\\in(0,2/L] and \\theta\\in[0,\\eta/2] (indeed, \\theta\\ge0 because \\eta\\le1/L, and \\theta\\le\\eta/2 because 1/L-\\eta\\le\\tfrac{1}{2}\\eta is equivalent to \\eta\\ge2/(3L)).\n\nLet t:=\\eta L and s:=\\theta L. With the above choice, s=1-t and t\\in[2/3,1). The iteration becomes\n\\[\n x_{n+1}=x_n-(\\eta+\\theta)Lx_n+\\theta Lx_{n-1}=(1-(t+s))x_n+s\\,x_{n-1}=(0)\\cdot x_n+s\\,x_{n-1}=s\\,x_{n-1}.\n\\]\nWith the initialization x_{-1}=x_0 and any x_0\\neq0, we get\n\\[\n x_1=sx_0\\neq0,\\qquad x_2=sx_0=x_1,\\qquad x_3=sx_1.\n\\]\nLet a_n:=f(x_n) and define forward differences \\Delta_n:=a_n-a_{n+1}. Convexity of the piecewise-linear interpolation of (a_n) is equivalent to \\Delta_{n+1}\\le\\Delta_n for all n. Here,\n\\[\n \\Delta_1=a_1-a_2=f(x_1)-f(x_2)=0,\n\\]\nwhile, using x_3=sx_1 and s=1-t,\n\\[\n \\Delta_2=a_2-a_3=\\tfrac{L}{2}\\bigl(x_1^2-s^2x_1^2\\bigr)=\\tfrac{L}{2}\\,(1-s^2)\\,x_1^2=\\tfrac{L}{2}\\,t(2-t)\\,x_1^2>0,\n\\]\nsince t\\in(0,2) and x_1\\neq0. Thus \\Delta_2>\\Delta_1, violating convexity. Therefore, for every \\eta\\in[2/(3L),1/L) with \\theta=1/L-\\eta and every x_0\\neq0, the piecewise-linear interpolation of (f(x_n)) is not convex."
  },
  {
    "statement": "Let L>0. There exist a convex L-smooth function f, a stepsize η with 1.75/L<η<2/L, an initialization x_0, and an index n≥0 such that, for the gradient-descent iterates x_{k+1}=x_k-η∇f(x_k) and Δ_n:=f(x_n)-f(x_{n+1}), one has\n$$\n\bigl(Δ_n-Δ_{n+1}\bigr)\n<\nη\bigl(1-\tfrac{ηL}{2}\bigr)\bigl(\\\nabla f(x_{n+1})\\|^2-\\|\\nabla f(x_{n+2})\\|^2\bigr).\n$$",
    "proof_markdown": "Proof. Let $g_n:=\\nabla f(x_n)$ and $Δ_n:=f(x_n)-f(x_{n+1})$. Suppose, for the sake of contradiction, that for every convex $L$–smooth $f$, every stepsize $0<η\\le2/L$, and all $n\\ge0$, the inequality\n$$\nΔ_n-Δ_{n+1}\\;\\ge\\;η\\Bigl(1-\\tfrac{ηL}{2}\\Bigr)\\bigl(\\|g_{n+1}\\|^2-\\|g_{n+2}\\|^2\\bigr)\n$$\nholds along the gradient-descent iterates $x_{k+1}=x_k-η\\nabla f(x_k)$.\n\nFix $L>0$ and choose $η\\in(1.75/L,2/L)$, so $c_η:=η\\bigl(1-\\tfrac{ηL}{2}\\bigr)>0$. Two standard facts apply:\n- For convex $L$–smooth $f$ and any $0<η\\le2/L$, the gradient norms along GD are nonincreasing, hence $\\|g_{n+1}\\|\\ge\\|g_{n+2}\\|$ and so $\\|g_{n+1}\\|^2-\\|g_{n+2}\\|^2\\ge0$.\n- There exist a convex $L$–smooth function $f$ and an initialization $x_0$ such that, for GD with this $η$, the sequence $n\\mapsto f(x_n)$ is not convex; equivalently, there exists $n\\ge0$ with $Δ_n-Δ_{n+1}<0$.\n\nFor this $f,η$, and index $n$, the assumed inequality yields\n$$\nΔ_n-Δ_{n+1}\\;\\ge\\;c_η\\bigl(\\|g_{n+1}\\|^2-\\|g_{n+2}\\|^2\\bigr)\\;\\ge\\;0,\n$$\nwhich contradicts $Δ_n-Δ_{n+1}<0$. Hence the universal inequality cannot hold. Therefore there exist convex $L$–smooth $f$, a stepsize $η\\in(1.75/L,2/L)$, an initialization $x_0$, and an index $n\\ge0$ for which\n$$\nΔ_n-Δ_{n+1}\n<η\\Bigl(1-\\tfrac{ηL}{2}\\Bigr)\\bigl(\\|\\nabla f(x_{n+1})\\|^2-\\|\\nabla f(x_{n+2})\\|^2\\bigr),\n$$\nas claimed."
  }
]