[
  {
    "statement": "Non-monotonicity of per-iteration decrease ratios. There exist 0<\\mu<L, a stepsize \\eta\\in(0,2/L), a \\mu-strongly convex and L-smooth function f, and an initialization x_0 such that for the gradient descent iterates x_{n+1}=x_n-\\eta\\nabla f(x_n), with r_n:=\\tfrac{f(x_{n+1})-f_\\star}{f(x_n)-f_\\star} and q:=\\max_{\\lambda\\in[\\mu,L]}|1-\\eta\\lambda|^2, the sequence \\{r_n\\} is strictly increasing (hence not nonincreasing). Moreover, in this example \\lim_{n\\to\\infty} r_n=q.",
    "proof_markdown": "Proof (by contradiction). Assume that for every \\mu-strongly convex and L-smooth function f and every \\eta\\in(0,2/L), along gradient descent the ratios r_n:=\\tfrac{f(x_{n+1})-f_\\star}{f(x_n)-f_\\star} are nonincreasing.\n\nConsider the quadratic in dimension 2:\n$$\n f(x)=\\tfrac12\\big(\\mu x_1^2+L x_2^2\\big),\\qquad 0<\\mu<L.\n$$\nThen f is \\mu-strongly convex and L-smooth, with unique minimizer x_\\star=0 and f_\\star=f(x_\\star)=0. Gradient descent with stepsize \\eta\\in(0,2/L) evolves coordinate-wise as\n$$\n x_{n+1,1}=(1-\\eta\\mu)\\,x_{n,1},\\qquad x_{n+1,2}=(1-\\eta L)\\,x_{n,2}.\n$$\nLet $m_1:=1-\\eta\\mu$, $m_2:=1-\\eta L$, and $\\rho_i:=m_i^2\\in[0,1)$, $i=1,2$. For any initialization $x_0=(a,b)$ with $a,b\\ne0$, set $c_1:=\\mu a^2>0$, $c_2:=L b^2>0$, and define $a_n:=c_1\\rho_1^{\\,n}+c_2\\rho_2^{\\,n}$. Then\n$$\n f(x_n)-f_\\star=\\tfrac12 a_n,\\qquad r_n=\\frac{a_{n+1}}{a_n}.\n$$\nBy Cauchy–Schwarz,\n$$\n a_{n+1}^2=\\Big(\\sum_{i=1}^2\\sqrt{c_i}\\,\\rho_i^{\\,n/2}\\cdot\\sqrt{c_i}\\,\\rho_i^{\\,n/2+1}\\Big)^2\\le\\Big(\\sum_{i=1}^2 c_i\\rho_i^{\\,n}\\Big)\\Big(\\sum_{i=1}^2 c_i\\rho_i^{\\,n+2}\\Big)=a_n a_{n+2},\n$$\nwith strict inequality whenever $\\rho_1\\ne\\rho_2$ and $c_1,c_2>0$. Hence $r_n\\le r_{n+1}$ for all n, and in fact $r_n<r_{n+1}$ for all n under $\\rho_1\\ne\\rho_2$ and $a,b\\ne0$. This contradicts the assumed nonincreasing property. Therefore, there exist \\mu, L, \\eta, f, and x_0 for which \\{r_n\\} is strictly increasing.\n\nFinally, in this quadratic example,\n- if $\\rho_1\\ne\\rho_2$ and, say, $\\rho_2>\\rho_1$, then\n$$\n r_n=\\frac{c_1\\rho_1^{\\,n+1}+c_2\\rho_2^{\\,n+1}}{c_1\\rho_1^{\\,n}+c_2\\rho_2^{\\,n}}=\\rho_2\\,\\frac{c_2+c_1(\\rho_1/\\rho_2)^{n+1}}{c_2+c_1(\\rho_1/\\rho_2)^n}\\longrightarrow \\rho_2=\\max\\{\\rho_1,\\rho_2\\};\n$$\n- if $\\rho_1=\\rho_2=:\\rho$, then $r_n\\equiv\\rho$.\n\nSince $\\lambda\\mapsto |1-\\eta\\lambda|^2$ is convex on $[\\mu,L]$, its maximum over $[\\mu,L]$ is attained at an endpoint, so\n$$\n q=\\max_{\\lambda\\in[\\mu,L]}|1-\\eta\\lambda|^2=\\max\\{|1-\\eta\\mu|^2,|1-\\eta L|^2\\}=\\max\\{\\rho_1,\\rho_2\\}.\n$$\nThus $\\lim_{n\\to\\infty} r_n=q$ in this example. ∎"
  },
  {
    "statement": "A stronger counterexample holds in one dimension: let $f(x)=\\tfrac12 x^2$, which is convex and $1$-smooth. Then for any fixed stepsize $\\eta\\in[\\tfrac32,2)$ and any nonzero initialization $x_0$, the gradient-descent iterates $x_{n+1}=x_n-\\eta\\nabla f(x_n)$ with averages $\\bar x_n=\\tfrac1{n+1}\\sum_{k=0}^n x_k$ produce a sequence $n\\mapsto f(\\bar x_n)$ that is not convex. Moreover, the gradient norms satisfy $\\|g_n\\|=\\|\\nabla f(x_n)\\|>0$ and are strictly decreasing to $0$.",
    "proof_markdown": "Work in one dimension and fix $f(x)=\\tfrac12 x^2$, which is convex and $1$-smooth. For any stepsize $\\eta\\in[\\tfrac32,2)$, let $\\rho:=1-\\eta\\in(-1,-\\tfrac12]$ and run gradient descent\n$$\nx_{n+1}=x_n-\\eta\\nabla f(x_n)=(1-\\eta)x_n=\\rho\\,x_n,\n$$\nso $x_n=\\rho^n x_0$ for all $n\\ge0$. The Cesàro averages are\n$$\n\\bar x_n=\\frac1{n+1}\\sum_{k=0}^n x_k=x_0\\,\\frac{1-\\rho^{n+1}}{(n+1)(1-\\rho)}=:x_0\\,s_n.\n$$\nBecause $|\\rho|<1$, we have $1-\\rho>0$ and $1-\\rho^{n+1}>0$, hence $s_n>0$ for all $n$.\n\nConsequently,\n$$\nf(\\bar x_n)=\\tfrac12 x_0^2 s_n^2.\n$$\nFor a real sequence, convexity is equivalent to nondecreasing forward differences $\\Delta_n:=f(\\bar x_{n+1})-f(\\bar x_n)$. Here\n$$\n\\Delta_n=\\tfrac12 x_0^2\\big(s_{n+1}^2-s_n^2\\big)=\\tfrac12 x_0^2\\,(s_{n+1}-s_n)(s_{n+1}+s_n),\n$$\nso $\\operatorname{sign}(\\Delta_n)=\\operatorname{sign}(s_{n+1}-s_n)$ because $s_{n+1}+s_n>0$.\n\nCompute\n$$\ns_1=\\frac{1+\\rho}{2},\\quad s_2=\\frac{1+\\rho+\\rho^2}{3},\\quad s_3=\\frac{1+\\rho+\\rho^2+\\rho^3}{4},\n$$\nso\n$$\ns_2-s_1=\\frac{2\\rho^2-\\rho-1}{6}=\\frac{(2\\rho+1)(\\rho-1)}{6},\\qquad\ns_3-s_2=\\frac{3\\rho^3-\\rho^2-\\rho-1}{12}=\\frac{(\\rho-1)(3\\rho^2+2\\rho+1)}{12}.\n$$\nFor $\\rho\\in(-1,-\\tfrac12]$, we have $\\rho-1<0$, $2\\rho+1\\le0$, and $3\\rho^2+2\\rho+1>0$ (its discriminant is $4-12<0$), hence\n$$\ns_2-s_1\\ge0\\quad\\text{and}\\quad s_3-s_2<0.\n$$\nTherefore $\\Delta_1\\ge0$ while $\\Delta_2<0$, so the forward differences are not nondecreasing. It follows that the sequence $n\\mapsto f(\\bar x_n)$ is not convex. This conclusion holds for every nonzero $x_0$ because $x_0^2$ is a positive factor in all $\\Delta_n$.\n\nFinally, the gradients are $g_n=\\nabla f(x_n)=x_n=\\rho^n x_0$, so $\\|g_n\\|=|\\rho|^n\\,|x_0|>0$ for all $n$ and $\\|g_n\\|$ is strictly decreasing to $0$ since $|\\rho|<1$. Thus the nonconvexity occurs even though the gradient norms are never zero and decay strictly."
  },
  {
    "statement": "Let f(x)=\\tfrac12 x^\\top H x+b^\\top x with H\\succeq 0. For any constant stepsize \\eta\\ge 0 satisfying 2I-\\eta H\\succeq 0 (equivalently, 0\\le \\eta\\le 2/\\lambda_{\\max}(H)), the sequence k\\mapsto f(x_k) generated by gradient descent x_{k+1}=x_k-\\eta\\nabla f(x_k) is nonincreasing and convex for every initialization x_0.",
    "proof_markdown": "Proof. Let f(x)=\\tfrac12 x^\\top H x+b^\\top x with H\\succeq 0, and fix a constant stepsize \\eta\\ge 0 such that 2I-\\eta H\\succeq 0 (equivalently, 0\\le\\eta\\le 2/\\lambda_{\\max}(H)). Run GD x_{k+1}=x_k-\\eta\\nabla f(x_k), and write g_k:=\\nabla f(x_k)=H x_k+b.\n\nBecause f is quadratic, the exact descent identity holds for all x and all \\eta\\in\\mathbb{R}:\n$$\n f(x-\\eta\\nabla f(x))=f(x)-\\eta\\,\\|\\nabla f(x)\\|^2+\\tfrac{\\eta^2}{2}\\,\\nabla f(x)^\\top H\\,\\nabla f(x).\n$$\nApplying this at x=x_k gives\n$$\n\\Delta_k:=f(x_k)-f(x_{k+1})=\\frac{\\eta}{2}\\,g_k^\\top(2I-\\eta H)\\,g_k.\n$$\nSince \\eta\\ge 0 and 2I-\\eta H\\succeq 0, we have \\Delta_k\\ge 0 for all k, so k\\mapsto f(x_k) is nonincreasing.\n\nMoreover, the gradients evolve linearly:\n$$\n g_{k+1}=\\nabla f(x_{k+1})=H(x_k-\\eta g_k)+b=(I-\\eta H)g_k.\n$$\nTherefore, using that polynomials in H commute,\n$$\n\\Delta_{k+1}=\\frac{\\eta}{2}\\,g_{k+1}^\\top(2I-\\eta H)g_{k+1}\n=\\frac{\\eta}{2}\\,g_k^\\top(2I-\\eta H)(I-\\eta H)^2 g_k.\n$$\nSince H\\succeq 0 and 2I-\\eta H\\succeq 0, every eigenvalue \\lambda of H satisfies 0\\le \\eta\\lambda\\le 2, hence the spectrum of I-\\eta H lies in [\\!-1,1]. Thus\n$$\n0\\preceq (I-\\eta H)^2\\preceq I.\n$$\nBecause (2I-\\eta H) and (I-\\eta H)^2 commute and (2I-\\eta H)\\succeq 0, we obtain\n$$\n\\Delta_{k+1}\\;\\le\\;\\frac{\\eta}{2}\\,g_k^\\top(2I-\\eta H)g_k\\;=\\;\\Delta_k,\n$$\nso {\\Delta_k} is nonincreasing and the optimization curve k\\mapsto f(x_k) is convex. Equivalently, the discrete second differences are nonnegative; indeed,\n$$\n\\Delta_k-\\Delta_{k+1}\n=\\frac{\\eta}{2}\\,g_k^\\top(2I-\\eta H)\\bigl[I-(I-\\eta H)^2\\bigr]g_k\n=\\frac{\\eta^2}{2}\\,g_k^\\top(2I-\\eta H)H(2I-\\eta H)g_k\\;\\ge\\;0,\n$$\nwhere we used $I-(I-\\eta H)^2=\\eta H(2I-\\eta H)$ and that these matrices commute. This holds for every initialization $x_0$ and every such stepsize $\\eta$."
  },
  {
    "statement": "Stronger nonexistence claim. Fix any L>0 and consider the one-dimensional convex L-smooth quadratic f(x)=\\tfrac{L}{2}x^2. For any nonzero initialization x_0 and any legitimate step \\eta_0\\in(0,\\tfrac{2}{L}] with \\alpha_0:=L\\eta_0\\in\\bigl(1-\\tfrac{1}{\\sqrt{2}},\\,1+\\tfrac{1}{\\sqrt{2}}\\bigr), the feasibility set defining\n$$\\widehat\\eta_1:=\\sup\\{\\eta\\in(0,\\tfrac{2}{L}]:\\ f(x_1)-f(x_2(\\eta))\\ge f(x_0)-f(x_1)\\},\\quad x_2(\\eta)=x_1-\\eta g_1,$$\nis empty (hence \\widehat\\eta_1 does not exist). Equivalently, for this f and any x_0\\ne 0, the set is empty if and only if \\alpha_0\\in\\bigl(1-\\tfrac{1}{\\sqrt{2}},\\,1+\\tfrac{1}{\\sqrt{2}}\\bigr). In particular, choosing \\alpha_0=\\tfrac{3}{2} (i.e., \\eta_0=\\tfrac{3}{2L}) makes the set empty. Consequently, the prescription \\eta_n\\equiv\\widehat\\eta_n is not, in general, well-defined and therefore need not produce a convex optimization curve nor match the worst-case rate of the best fixed step size chosen in hindsight.",
    "proof_markdown": "Fix arbitrary L>0 and consider f(x)=\\tfrac{L}{2}x^2 on \\mathbb{R}. Let x_0\\neq 0 and take a legitimate \\eta_0\\in(0,\\tfrac{2}{L}], with \\alpha_0:=L\\eta_0\\in(0,2]. The first GD step gives x_1=(1-\\alpha_0)x_0 and g_1=\\nabla f(x_1)=Lx_1.\n\nFor any candidate second-step \\eta\\in(0,\\tfrac{2}{L}], write \\beta:=L\\eta\\in(0,2], so x_2(\\eta)=x_1-\\eta g_1=(1-\\beta)x_1. Define the one-step decrease at point x with dimensionless step \\theta\\in(0,2] as\n$$\\Delta(x;\\theta):=f(x)-f\\bigl((1-\\theta)x\\bigr)=\\frac{L}{2}\\,\\varphi(\\theta)\\,x^2,\\qquad \\varphi(t):=2t-t^2.$$\nThen\n$$\\Delta_0:=f(x_0)-f(x_1)=\\tfrac{L}{2}\\,\\varphi(\\alpha_0)\\,x_0^2,$$\n$$\\Delta_1(\\beta):=f(x_1)-f\\bigl(x_2(\\eta)\\bigr)=\\tfrac{L}{2}\\,\\varphi(\\beta)\\,x_1^2=\\tfrac{L}{2}\\,\\varphi(\\beta)\\,(1-\\alpha_0)^2\\,x_0^2.$$\nSince \\varphi is concave on [0,2] with maximum value 1 attained at \\beta=1, we have\n$$\\sup_{\\beta\\in(0,2]}\\Delta_1(\\beta)=\\tfrac{L}{2}\\,(1-\\alpha_0)^2\\,x_0^2.$$\nTherefore the feasibility set\n$$\\{\\eta\\in(0,\\tfrac{2}{L}]:\\ f(x_1)-f(x_2(\\eta))\\ge f(x_0)-f(x_1)\\}$$\nis nonempty if and only if\n$$\\sup_{\\beta\\in(0,2]}\\Delta_1(\\beta)\\;\\ge\\;\\Delta_0\\quad\\Longleftrightarrow\\quad (1-\\alpha_0)^2\\;\\ge\\;\\varphi(\\alpha_0)=2\\alpha_0-\\alpha_0^2.$$\nEquivalently,\n$$2\\alpha_0^2-4\\alpha_0+1\\;\\ge\\;0\\quad\\Longleftrightarrow\\quad (\\alpha_0-1)^2\\;\\ge\\;\\tfrac{1}{2}\\quad\\Longleftrightarrow\\quad \\alpha_0\\in(-\\infty,1-\\tfrac{1}{\\sqrt{2}}]\\cup[1+\\tfrac{1}{\\sqrt{2}},\\infty).$$\nSince \\alpha_0\\in(0,2], it follows that the feasibility set is empty if and only if\n$$\\alpha_0\\in\\bigl(1-\\tfrac{1}{\\sqrt{2}},\\,1+\\tfrac{1}{\\sqrt{2}}\\bigr).$$\nIn particular, for \\alpha_0=\\tfrac{3}{2} we have emptiness at step n=1, so \\widehat\\eta_1 does not exist. Hence there are legitimate initializations for which the rule \\eta_n\\equiv\\widehat\\eta_n is ill-defined; consequently, it cannot be guaranteed to produce a convex optimization curve nor to match the worst-case rate of the best fixed step size chosen in hindsight. ∎"
  },
  {
    "statement": "Let f: \\mathbb{R}^d \\to \\mathbb{R} be convex with L-Lipschitz gradient (i.e., L-smooth). Run gradient descent with constant step size \\(\\eta \\in (0,\\tfrac{2}{L}]\\): \\(x_{n+1}=x_n-\\eta\\nabla f(x_n)\\), and set \\(f_n=f(x_n)\\). Define the two-step smoothing \\(s_n:=\\tfrac12\\,(f_n+f_{n+1})\\). Then the sequence \\((s_n)\\) is convex in \\(n\\) and nonincreasing.",
    "proof_markdown": "\\begin{proof}\nLet $f:\\mathbb{R}^d\\to\\mathbb{R}$ be convex and $L$-smooth, and run gradient descent with constant step size $\\eta\\in(0,\\tfrac{2}{L}]$:\n$$\n x_{n+1}=x_n-\\eta\\nabla f(x_n),\\qquad f_n:=f(x_n),\\qquad g_n:=\\nabla f(x_n).\n$$\nDefine $s_n:=\\tfrac12(f_n+f_{n+1})$.\n\n1) Monotonicity of $(s_n)$. By the descent lemma, for any $k\\ge0$,\n$$\n f_{k+1}\\;\\le\\; f_k-\\tfrac{\\eta}{2}(2-\\eta L)\\,\\|g_k\\|^2\\;\\le\\; f_k.\n$$\nHence $(f_n)$ is nonincreasing, so $f_{n+2}\\le f_n$ and therefore\n$$\n s_n-s_{n+1}=\\tfrac12\\,(f_n-f_{n+2})\\;\\ge\\;0.\n$$\nThus $(s_n)$ is nonincreasing.\n\n2) Convexity of $(s_n)$. The sequence $(s_n)$ is convex iff\n$$\n s_n-s_{n+1}=\\tfrac12\\,(f_n-f_{n+2})\\;\\ge\\;\\tfrac12\\,(f_{n+1}-f_{n+3})=s_{n+1}-s_{n+2},\n$$\nthat is, iff for all $n\\ge0$,\n$$\n f_n-f_{n+2}\\;\\ge\\; f_{n+1}-f_{n+3}. \\tag{1}\n$$\nApply the descent lemma at indices $n$ and $n+2$ and subtract:\n$$\n f_{n+1}-f_{n+3}\\;\\le\\;\\big(f_n-f_{n+2}\\big)\\;-\n \\tfrac{\\eta}{2}(2-\\eta L)\\big(\\|g_n\\|^2-\\|g_{n+2}\\|^2\\big).\n$$\nEquivalently,\n$$\n \\big(f_n-f_{n+2}\\big)-\\big(f_{n+1}-f_{n+3}\\big)\\;\\ge\\;\\tfrac{\\eta}{2}(2-\\eta L)\\big(\\|g_n\\|^2-\\|g_{n+2}\\|^2\\big). \\tag{2}\n$$\nSince $f$ is convex and $L$-smooth, $\\nabla f$ is $1/L$-cocoercive (Baillon–Haddad):\n$$\n \\langle g_{k}-g_{k+1},\\, x_k-x_{k+1}\\rangle\\;\\ge\\;\\tfrac1L\\,\\|g_{k}-g_{k+1}\\|^2.\n$$\nUsing $x_k-x_{k+1}=\\eta g_k$ gives $\\langle g_k, g_k-g_{k+1}\\rangle\\ge\\tfrac{1}{\\eta L}\\|g_k-g_{k+1}\\|^2$. Hence\n$$\n\\|g_{k+1}\\|^2 = \\|g_k\\|^2 - 2\\langle g_k,g_k-g_{k+1}\\rangle + \\|g_k-g_{k+1}\\|^2\n\\le \\|g_k\\|^2 - \\Big(\\tfrac{2}{\\eta L}-1\\Big)\\|g_k-g_{k+1}\\|^2 \\le \\|g_k\\|^2,\n$$\nfor $\\eta\\le 2/L$. Thus $\\|g_{k+1}\\|\\le\\|g_k\\|$, and in particular $\\|g_{n+2}\\|\\le\\|g_n\\|$. With $(2-\\eta L)\\ge0$, the right-hand side of (2) is nonnegative, and (1) follows. (When $\\eta=2/L$, the factor $(2-\\eta L)$ is zero, so (1) follows from (2) even without using the monotonicity of $\\|g_k\\|$.)\n\nTherefore $(s_n)$ is convex and nonincreasing for all $\\eta\\in(0,\\tfrac{2}{L}]$.\n\\end{proof}"
  },
  {
    "statement": "Fix L>0. For the convex L-smooth quadratic f(x)=\\tfrac{L}{2}\\|x\\|^2, any initialization x_0\\ne 0, and any stepsize \\eta\\in(1/L,2/L), let the gradient-descent iterates be x_{n+1}=x_n-\\eta\\nabla f(x_n), and define the quadratic model \\(Q_n(y)=f(x_n)+\\langle\\nabla f(x_n),y-x_n\\rangle+\\tfrac{1}{2\\eta}\\|y-x_n\\|^2\\) and the model gaps \\(\\delta_n:=Q_n(x_{n+1})-f(x_{n+1})\\). Then {\\delta_n} is strictly increasing (hence not nonincreasing), and in fact the sequence n\\mapsto\\delta_n is strictly concave (i.e., \\(\\delta_{n+2}-2\\delta_{n+1}+\\delta_n<0\\)), so it is not convex.",
    "proof_markdown": "Consider f(x)=\\tfrac{L}{2}\\|x\\|^2, which is convex and L-smooth, and fix any x_0\\ne 0. For a stepsize \\eta\\in(1/L,2/L), gradient descent yields\n\\[\n x_{n+1}=x_n-\\eta\\nabla f(x_n)=x_n-\\eta L x_n=(1-\\eta L)x_n=:\\rho x_n,\n\\]\nso that \\(\\rho\\in(-1,0)\\), \\(x_n=\\rho^n x_0\\), \\(g_n:=\\nabla f(x_n)=L\\rho^n x_0\\), and \\(f_n:=f(x_n)=\\tfrac{L}{2}\\rho^{2n}\\|x_0\\|^2\\).\n\nThe quadratic model at \\(x_n\\) is\n\\[\n Q_n(y)=f_n+\\langle g_n,y-x_n\\rangle+\\tfrac{1}{2\\eta}\\|y-x_n\\|^2.\n\\]\nEvaluating at \\(y=x_{n+1}=x_n-\\eta g_n\\),\n\\[\n Q_n(x_{n+1})=f_n-\\eta\\|g_n\\|^2+\\tfrac{\\eta}{2}\\|g_n\\|^2=f_n-\\tfrac{\\eta}{2}\\|g_n\\|^2.\n\\]\nSince \\(\\|g_n\\|^2=L^2\\|x_n\\|^2=2L f_n\\), we obtain\n\\[\n Q_n(x_{n+1})=f_n-\\eta L f_n=(1-\\eta L)f_n=\\rho f_n,\\qquad f_{n+1}=\\rho^2 f_n.\n\\]\nHence\n\\[\n \\delta_n=Q_n(x_{n+1})-f_{n+1}=\\rho f_n-\\rho^2 f_n=\\rho(1-\\rho)f_n=\\frac{\\eta L^2}{2}\\,\\rho^{2n+1}\\|x_0\\|^2.\n\\]\nBecause \\(\\rho\\in(-1,0)\\), we have \\(\\delta_n<0\\) for all n. Moreover,\n\\[\n \\delta_{n+1}-\\delta_n=(\\rho^2-1)\\,\\delta_n>0,\n\\]\nsince \\(\\rho^2-1<0\\) and \\(\\delta_n<0\\). Thus {\\delta_n} is strictly increasing, so it is not nonincreasing.\n\nFor second differences,\n\\[\n \\delta_{n+2}-2\\delta_{n+1}+\\delta_n=(1-\\rho^2)^2\\,\\delta_n<0,\n\\]\nbecause \\(1-\\rho^2>0\\) and \\(\\delta_n<0\\). Therefore the sequence n\\mapsto\\delta_n is strictly concave, in particular not convex. ∎"
  },
  {
    "statement": "Let L>0. For every stepsize $\\eta\\in(0,\\tfrac{2}{L})$ there exist a one-dimensional convex $L$-smooth function $f$ and an initialization $x_0$ such that, for gradient descent $x_{n+1}=x_n-\\eta f'(x_n)$ with $g_n:=f'(x_n)$, the forward differences increase at $n=0$: $\\|g_0\\|-\\|g_1\\|<\\|g_1\\|-\\|g_2\\|$.",
    "proof_markdown": "Proof. Assume, for contradiction, that there exists a stepsize $\\eta\\in(0,\\tfrac{2}{L})$ such that for every convex $L$-smooth function $f$ and every initialization, gradient descent with this $\\eta$ produces a sequence $\\{\\|g_n\\|\\}$ whose forward differences are nonincreasing, i.e., $\\|g_n\\|-\\|g_{n+1}\\|\\ge \\|g_{n+1}\\|-\\|g_{n+2}\\|$ for all $n$.\n\nFix such $\\eta$ and $L>0$. Choose any $g_0>0$ and any $x_0\\in\\mathbb{R}$. Define\n$$x_1:=x_0-\\eta g_0,\\qquad x_2:=x_1-\\eta g_0,$$\nand define $f':\\mathbb{R}\\to\\mathbb{R}$ by\n$$\n f'(x):=\\begin{cases}\n g_0, & x\\ge x_1,\\\\[4pt]\n g_0+L(x-x_1), & x\\in[x_2,x_1],\\\\[4pt]\n g_2, & x\\le x_2,\n \\end{cases}\n$$\nwhere $g_2:=g_0+L(x_2-x_1)=g_0-L\\eta g_0=(1-\\eta L)g_0$. Then $f'$ is continuous, nondecreasing, and $L$-Lipschitz (its a.e. derivative takes values in $\\{0,L\\}$). Hence its antiderivative $f\\in C^{1,1}$ is convex and $L$-smooth.\n\nRun gradient descent $x_{n+1}=x_n-\\eta f'(x_n)$ and denote $g_n:=f'(x_n)$. By construction, $x_1=x_0-\\eta g_0$ and $f'(x)\\equiv g_0$ on $[x_1,\\infty)$, so $g_0=f'(x_0)$ and $g_1=f'(x_1)=g_0$. Moreover, on $[x_2,x_1]$ we have $f''\\equiv L$ a.e., hence\n$$\n g_2=f'(x_2)=f'(x_1)+\\int_{x_1}^{x_2}L\\,dt=g_0-L(x_1-x_2)=g_0-L\\eta g_1=(1-\\eta L)g_1.\n$$\nTherefore\n$$\n \\|g_0\\|-\\|g_1\\|=g_0-g_0=0,\\qquad\n \\|g_1\\|-\\|g_2\\|=g_0-|1-\\eta L|\\,g_0=(1-|1-\\eta L|)g_0.\n$$\nBecause $0<\\eta L<2$, we have $|1-\\eta L|<1$, so $\\|g_1\\|-\\|g_2\\|>0=\\|g_0\\|-\\|g_1\\|$. Thus the forward differences are not nonincreasing at $n=0$, contradicting the assumption.\n\nHence no such $\\eta$ exists. Equivalently, for every $\\eta\\in(0,\\tfrac{2}{L})$ there exist a convex $L$-smooth $f$ and an initialization for which the forward differences of $\\{\\|g_n\\|\\}$ increase at $n=0$ (indeed, $\\|g_0\\|-\\|g_1\\|<\\|g_1\\|-\\|g_2\\|$). ∎"
  },
  {
    "statement": "Let f be convex and L-smooth. Run gradient descent with any stepsize $\\eta\\in(\\tfrac{7}{4L},\\tfrac{2}{L})$ and restart (set $x_{n+1}=x_n$ and reset the comparison threshold) whenever $\\Delta_n>\\Delta_{n-1}$, where $\\Delta_n:=f(x_n)-f(x_n-\\eta\\nabla f(x_n))$. Then, between restarts, the sequence $n\\mapsto f(x_n)$ is convex. Moreover, there is no universal constant $C$ such that, for all convex $L$-smooth $f$ and all $\\varepsilon>0$, the number of iterations required by this restarted scheme to achieve $f(x_n)-f_\\star\\le\\varepsilon$ is at most $C$ times that of the best fixed stepsize $\\eta\\in(0,\\tfrac{2}{L}]$ chosen in hindsight.",
    "proof_markdown": "\\begin{proof}\nFix $L>0$ and let $f:\\mathbb{R}^d\\to\\mathbb{R}$ be convex and $L$-smooth. Fix a stepsize $\\eta\\in\\big(\\tfrac{7}{4L},\\tfrac{2}{L}\\big)$. Define tentative updates $\\widetilde x_{n+1}:=x_n-\\eta\\nabla f(x_n)$ and attempted decreases $\\Delta_n:=f(x_n)-f(\\widetilde x_{n+1})$. The scheme accepts step $n$ iff $\\Delta_n\\le \\Delta_{n-1}$ (with the convention that at the beginning of each block of accepted steps the comparison threshold is $+\\infty$); if rejected, it restarts by setting $x_{n+1}:=x_n$ and reinitializes the comparison threshold.\n\nClaim 1 (convexity between restarts). Consider any maximal block $I=\\{a,\\dots,b\\}$ of consecutive accepted steps. For every $n\\in I$ with $n>a$, acceptance gives $\\Delta_n\\le \\Delta_{n-1}$. Since for accepted steps $f(x_n)-f(x_{n+1})=\\Delta_n$, the forward differences $f(x_n)-f(x_{n+1})$ are nonincreasing on $I$. Hence the discrete optimization curve $n\\mapsto f(x_n)$ is convex on $I$.\n\nClaim 2 (no universal rate-preserving constant). Assume, toward a contradiction, that there exists a universal constant $C>0$ such that for every convex $L$-smooth $f$, every $\\varepsilon>0$, every starting point, and every choice of $\\eta\\in(\\tfrac{7}{4L},\\tfrac{2}{L})$, the restarted scheme reaches $f(x_n)-f_\\star\\le\\varepsilon$ in at most $C$ times as many iterations as the best fixed stepsize gradient descent with $\\eta\\in(0,\\tfrac{2}{L}]$ chosen in hindsight.\n\nTake $d=1$ and $f(x)=\\tfrac{L}{2}x^2$, whose unique minimizer is $x_\\star=0$. Let $x_0\\ne 0$ and fix any target $\\varepsilon\\in(0,f(x_0))$. For any $\\delta\\in(0,\\tfrac14)$ set $\\eta=\\tfrac{2-\\delta}{L}\\in\\big(\\tfrac{7}{4L},\\tfrac{2}{L}\\big)$. Plain gradient descent with this $\\eta$ satisfies\n$$\nx_{n+1}=(1-\\eta L)x_n=-(1-\\delta)\\,x_n,\\qquad f(x_n)=\\tfrac{L}{2}(1-\\delta)^{2n}x_0^2.\n$$\nThe attempted decreases obey\n$$\n\\Delta_n=f(x_n)-f(x_{n+1})=f(x_n)\\big(1-(1-\\delta)^2\\big),\\qquad \\frac{\\Delta_{n+1}}{\\Delta_n}=(1-\\delta)^2\\in(0,1),\n$$\nso $\\{\\Delta_n\\}$ is strictly decreasing. Therefore the restart condition $\\Delta_n>\\Delta_{n-1}$ never occurs, and the realized run coincides with plain GD at stepsize $\\eta$. Let $K$ be the first index with $f(x_K)\\le\\varepsilon$. Then\n$$\n(1-\\delta)^{2K}\\le \\frac{\\varepsilon}{f(x_0)} \\quad\\Longrightarrow\\quad K\\;\\ge\\;\\frac{\\log\\big(f(x_0)/\\varepsilon\\big)}{2\\,\\log\\big(1/(1-\\delta)\\big)}\\;\\ge\\;\\frac{1-\\delta}{2\\delta}\\,\\log\\!\\Big(\\frac{f(x_0)}{\\varepsilon}\\Big),\n$$\nwhere we used $\\log(1/(1-\\delta))\\le \\delta/(1-\\delta)$. In contrast, for the same $f$ the fixed stepsize $\\eta_\\star=1/L\\in(0,2/L]$ attains $x_1=0$ and hence reaches $\\varepsilon$ in one step; thus the best fixed-stepsize complexity in hindsight is $k_{\\mathrm{best}}(\\varepsilon)=1$. Since $K\\to\\infty$ as $\\delta\\downarrow 0$, for the assumed constant $C>0$ we can choose $\\delta\\in(0,\\tfrac14)$ so small that $K> C\\,k_{\\mathrm{best}}(\\varepsilon)=C$, contradicting the assumption.\n\nCombining the two claims proves that the restart rule enforces convexity of the optimization curve on each block between restarts, but no universal constant-factor comparison with the best fixed stepsize method holds.\n\\end{proof}"
  }
]