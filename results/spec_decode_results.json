[
  {
    "statement": "Fix block size K and proposal conditionals (p_{n:n+K-1}). Among all unbiased K-block draft–verify schemes, the longest-prefix verifier first-order stochastically dominates any other valid scheme: for all j\\in\\{0,\\dots,K\\}, \\mathbb{P}_{\\mathrm{LP}}[A^{(K)}\\ge j] \\ge \\mathbb{P}_{\\text{any valid}}[A^{(K)}\\ge j]. In particular, it maximizes \\mathbb{E}[A^{(K)}].",
    "proof_markdown": "**Proof.** Fix an iteration index $n$ and a realized history $h\\equiv x_{1:n-1}$. For $\\tau\\in\\{1,\\dots,K\\}$ let\n$$\nP_p^{(\\tau)}(x_{1:\\tau}\\mid h) \\,=\\, \\prod_{i=1}^{\\tau} p_{n+i-1}(x_i\\mid h, x_{1:i-1}),\\qquad\nP_q^{(\\tau)}(x_{1:\\tau}\\mid h) \\,=\\, \\prod_{i=1}^{\\tau} q_{n+i-1}(x_i\\mid h, x_{1:i-1})\n$$\ndenote the $\\tau$-step path laws of the proposal and target, respectively. Consider any unbiased (valid) $K$-block draft–verify scheme. By exactness, its (eventual) output law for the next $K$ tokens given $h$ is $P_q^{(K)}(\\cdot\\mid h)$. Therefore the scheme induces a coupling $\\Gamma_h$ of two random blocks $(X_{1:K},Y_{1:K})\\in\\mathcal V^K\\times\\mathcal V^K$ with marginals $P_p^{(K)}(\\cdot\\mid h)$ and $P_q^{(K)}(\\cdot\\mid h)$, where $X_{1:K}$ is the drafted block and $Y_{1:K}$ is the scheme’s (eventual) output block.\n\nLet the first-disagreement time be\n$$\n\\sigma\\,:=\\,\\inf\\{j\\ge 1: X_j\\ne Y_j\\}\\ (\\inf\\emptyset:=\\infty),\\qquad L^{(K)}\\,:=\\,\\min\\{K,\\,\\sigma-1\\}.\n$$\nBy construction of the induced coupling (accepted tokens are committed as the prefix of the output), the accepted-prefix length $A^{(K)}$ of the scheme satisfies $A^{(K)}\\le L^{(K)}$ almost surely, with equality $A^{(K)}=L^{(K)}$ for the longest-prefix (LP) verifier.\n\nHence, for every $j\\in\\{1,\\dots,K\\}$ and realized $h$,\n$$\n\\mathbb P\\big(A^{(K)}\\ge j\\mid h\\big)\\ \\le\\ \\Gamma_h\\big(X_{1:j}=Y_{1:j}\\big)\n\\ \\le\\ 1-\\operatorname{TV}\\big(P_p^{(j)}(\\cdot\\mid h),P_q^{(j)}(\\cdot\\mid h)\\big)\n\\ =\\ \\sum_{x_{1:j}\\in\\mathcal V^j} \\min\\Big\\{P_p^{(j)}(x_{1:j}\\mid h),\\,P_q^{(j)}(x_{1:j}\\mid h)\\Big\\}.\n\\tag{1}\n$$\nThe first inequality uses $\\{A^{(K)}\\ge j\\}\\subseteq\\{X_{1:j}=Y_{1:j}\\}$ under $\\Gamma_h$; the second is the maximal-coupling bound applied to the distributions on $\\mathcal V^j$.\n\nNow average (1) over the random prefix $h\\sim q$ (the target’s prefix law): for any valid scheme,\n$$\n\\mathbb P\\big(A^{(K)}\\ge j\\big)\n\\ \\le\\ \\mathbb E_{h\\sim q}\\Big[\\sum_{x_{1:j}} \\min\\{P_p^{(j)}(x_{1:j}\\mid h),P_q^{(j)}(x_{1:j}\\mid h)\\}\\Big]\n\\ =:\\ S_j.\n\\tag{2}\n$$\nSumming (2) over $j=1,\\dots,K$ and using $\\mathbb E[A^{(K)}]=\\sum_{j=1}^K \\mathbb P(A^{(K)}\\ge j)$ gives, for any valid scheme,\n$$\n\\mathbb E\\big[A^{(K)}\\big] \\ \\le\\ \\sum_{j=1}^K S_j.\n\\tag{3}\n$$\nFor the LP verifier, the block-verification optimality formula yields the exact value\n$$\n\\mathbb E\\big[A^{(K)}_{\\mathrm{LP}}\\big] \\ =\\ \\sum_{j=1}^K S_j.\n\\tag{4}\n$$\nSince each inequality in (2) is an individual upper bound with nonnegative slack and the sum of these slacks equals zero for LP by (3)–(4), every slack must be zero. Thus, for every $j\\in\\{1,\\dots,K\\}$,\n$$\n\\mathbb P_{\\mathrm{LP}}\\big(A^{(K)}\\ge j\\big) \\ =\\ S_j \\ \\ge\\ \\mathbb P_{\\text{any valid}}\\big(A^{(K)}\\ge j\\big).\n$$\nFor $j=0$ the inequality is trivial. Hence the LP verifier first-order stochastically dominates every other unbiased $K$-block scheme, and in particular maximizes $\\mathbb E[A^{(K)}]$. Equivalently, the LP-induced coupling attains the maximal agreement probabilities $\\Gamma_h(X_{1:j}=Y_{1:j})=1-\\operatorname{TV}(P_p^{(j)}(\\cdot\\mid h),P_q^{(j)}(\\cdot\\mid h))$ simultaneously for all $j$ (cf. Völlering, 2016). ∎"
  },
  {
    "statement": "Local temperature-sensitivity law (pointwise, uniform over bounded support). Fix m∈ℕ and any ρ∈(0,1). For each step n and prefix x_{1:n−1}, let q_n(·|x_{1:n−1}) be a distribution supported on a finite set S(n,x_{1:n−1}) with |S(n,x_{1:n−1})|≤m. For |τ−1|≤ρ, define the temperature-scaled proposal on this support by\n$$p_{\\tau,n}(v)=\\frac{q_n(v)^{\\tau}}{\\sum_{u\\in S(n,x_{1:n-1})} q_n(u)^{\\tau}}\\quad(v\\in S(n,x_{1:n-1})),\\qquad p_{\\tau,n}(v)=0\\ (v\\notin S(n,x_{1:n-1})).$$\nThen, uniformly over all n and prefixes,\n$$\\operatorname{TV}(p_{\\tau,n},q_n)=\\tfrac{|\\tau-1|}{2}\\,\\mathbb{E}_{q_n}\\big[\\,|\\log q_n(V)-\\mathbb{E}_{q_n}[\\log q_n(V)]|\\,\\big]+O\\big((\\tau-1)^2\\big),$$\nwhere the O-constant depends only on m and ρ (and not on n, the prefix, or q_n). Consequently, averaging over prefixes yields the per-step expected rejection\n$$\\tau_n(\\tau)=\\mathbb{E}_{x_{1:n-1}}\\big[\\operatorname{TV}(p_{\\tau,n},q_n)\\big]=\\tfrac{|\\tau-1|}{2}\\,\\mathbb{E}_{x_{1:n-1}}\\Big[\\mathbb{E}_{q_n}\\big[\\,|\\log q_n(V)-\\mathbb{E}_{q_n}[\\log q_n(V)]|\\,\\big]\\Big]+O\\big((\\tau-1)^2\\big),$$\nwith the same O-constant.",
    "proof_markdown": "Proof. Fix a step n and prefix x_{1:n-1}. Write q≡q_n(·|x_{1:n-1}), let S:=supp(q), and assume |S|≤m. Fix any ρ∈(0,1) and restrict to |τ−1|≤ρ so that τ>0. Define, for v∈S,\n$$p_τ(v)=\\frac{q(v)^τ}{Z(τ)},\\qquad Z(τ):=\\sum_{u\\in S} q(u)^τ,\\quad\\text{and set }p_τ(v)=0\\text{ for }v\\notin S.$$\nLet ℓ(v):=\\log q(v) for v∈S and s_τ:=\\mathbb E_{p_τ}[ℓ]. Then, for v∈S,\n$$\\frac{\\mathrm d}{\\mathrm dτ}\\log p_τ(v)=ℓ(v)-s_τ=:h_τ(v),\\qquad \\Rightarrow\\qquad \\dot p_τ(v)=p_τ(v)\\,h_τ(v).$$\nDifferentiating again and using \\(\\dot s_τ=\\sum_{u\\in S}ℓ(u)\\dot p_τ(u)=\\operatorname{Var}_{p_τ}(ℓ)\\) gives, for v∈S,\n$$\\ddot p_τ(v)=p_τ(v)\\big(h_τ(v)^2-\\operatorname{Var}_{p_τ}(ℓ)\\big). \\tag{1}$$\nFor v∉S we have p_τ(v)≡0 for τ>0, hence \\(\\dot p_τ(v)=\\ddot p_τ(v)=0\\).\n\nTaylor’s formula with integral remainder about τ=1 yields, for δ:=τ−1 and all v,\n$$p_{1+δ}(v)=q(v)+δ\\,\\dot p_1(v)+r_v(δ),\\qquad r_v(δ)=\\int_0^{δ}(δ-t)\\,\\ddot p_{1+t}(v)\\,\\mathrm dt.$$\nSumming absolute values and using \\(\\big||a+b|-|a|\\big|\\le|b|\\) gives\n$$\\Big|\\sum_v|p_{1+δ}(v)-q(v)|-|δ|\\sum_v|\\dot p_1(v)|\\Big|\\le\\sum_v|r_v(δ)|\\le\\int_0^{|δ|}(|δ|-t)\\sum_v|\\ddot p_{1+t}(v)|\\,\\mathrm dt. \\tag{2}$$\nFrom (1) on S and zeros off S,\n$$\\sum_v|\\ddot p_τ(v)|\\le\\sum_{v\\in S} p_τ(v)\\big(h_τ(v)^2+\\operatorname{Var}_{p_τ}(ℓ)\\big)=2\\operatorname{Var}_{p_τ}(ℓ)\\le2\\,\\mathbb E_{p_τ}[ℓ^2].$$\nFor |τ−1|≤ρ<1, write a:=τ∈[1−ρ,1+ρ]. Then\n$$\\mathbb E_{p_a}[ℓ^2]=\\frac{\\sum_{v\\in S} q(v)^a\\,ℓ(v)^2}{\\sum_{v\\in S} q(v)^a}.$$\nWith q(v)=e^{−s} and s≥0, we have \\(q(v)^a\\,ℓ(v)^2=e^{−a s}s^2\\le\\max_{s\\ge0}s^2e^{−a s}=4/(a^2e^2)\\le4/((1−ρ)^2e^2)\\). Hence\n$$\\sum_{v\\in S} q(v)^a\\,ℓ(v)^2\\le \\frac{4|S|}{(1-ρ)^2e^2}.$$\nMoreover, for a∈[1−ρ,1] we have \\(\\sum_{v\\in S} q(v)^a\\ge1\\), and for a∈[1,1+ρ], \\(\\sum_{v\\in S} q(v)^a\\ge |S|^{1-a}\\ge |S|^{−ρ}\\). Thus for all a∈[1−ρ,1+ρ],\n$$\\sum_{v\\in S} q(v)^a\\ge |S|^{−ρ}.$$\nTherefore,\n$$\\sup_{|τ-1|\\leρ}\\ \\sum_v|\\ddot p_τ(v)|\\ \\le\\ 2\\sup_{|τ-1|\\leρ}\\mathbb E_{p_τ}[ℓ^2]\\ \\le\\ \\frac{8}{e^2}\\,\\frac{|S|^{1+ρ}}{(1-ρ)^2}\\ \\le\\ \\frac{8}{e^2}\\,\\frac{m^{1+ρ}}{(1-ρ)^2}=:C_{m,ρ}. \\tag{3}$$\nCombining (2)–(3) yields \\(\\sum_v|r_v(δ)|\\le (C_{m,ρ}/2)\\,δ^2\\) for |δ|≤ρ. At τ=1, p_1=q and, for v∈S, \\(\\dot p_1(v)=q(v)\\big(ℓ(v)-\\mathbb E_q[ℓ]\\big)\\) (while \\(\\dot p_1(v)=0\\) for v∉S), hence\n$$\\sum_v|\\dot p_1(v)|=\\sum_{v\\in S} q(v)\\,\\big|ℓ(v)-\\mathbb E_q[ℓ]\\big|=\\mathbb E_q\\big[\\,|\\log q(V)-\\mathbb E_q[\\log q(V)]|\\,\\big].$$\nUsing \\(\\operatorname{TV}(p,q)=\\tfrac12\\sum_v|p-q|\\), we have, uniformly for |τ−1|≤ρ,\n$$\\operatorname{TV}(p_τ,q)=\\frac{|τ-1|}{2}\\,\\mathbb E_q\\big[\\,|\\log q(V)-\\mathbb E_q[\\log q(V)]|\\,\\big]+O\\big((τ-1)^2\\big),$$\nwhere the O-constant depends only on (m,ρ) and not on q.\n\nThis establishes the asserted pointwise expansion, uniformly over all n and prefixes with |supp(q_n)|≤m. Averaging over prefixes (by linearity of expectation) yields the stated expansion for the per-step expected rejection with the same O-constant. ∎"
  },
  {
    "statement": "(Increasing-order optimality and stepwise characterization of SD) Fix a finite horizon T. Among all unbiased token-level draft–verify algorithms that use exactly one proposal per position with the same proposal conditionals (p_n), standard SD minimizes E[φ(N_{\\mathrm{rej}})] for every function φ that is nondecreasing on {0,1,...,T}. Equivalently, N_{\\mathrm{rej}}^{\\mathrm{SD}} is minimal in the increasing (stochastic) order among all such algorithms. In particular, SD minimizes the mean, the second moment, and all exponential moments of N_{\\mathrm{rej}}; consequently, within any subclass of algorithms having the same mean, it minimizes the variance. Moreover, any algorithm that is optimal for φ(t)=t (and hence any algorithm that is optimal for all such nondecreasing φ) must, at every step and every prefix, use a maximal-agreement coupling between q_n(\\cdot\\mid x) and p_n(\\cdot\\mid x), i.e., it must achieve P\\{X_n=Y_n\\mid x\\}=1-\\operatorname{TV}(q_n(\\cdot\\mid x),p_n(\\cdot\\mid x)).",
    "proof_markdown": "\\begin{proof}\nFix $T\\in\\mathbb N$ and a finite alphabet $\\mathcal V$. Let $q$ be the target AR model with conditionals $q_n(\\cdot\\mid x_{1:n-1})$ and let $p$ denote the draft conditionals $p_n(\\cdot\\mid x_{1:n-1})$. Consider any unbiased token-level draft–verify algorithm $A$ that uses exactly one proposal per position and the given $(p_n)$; unbiasedness means the output law is $q$. At step $n$, after the accepted prefix $X_{1:n-1}=x_{1:n-1}$, the algorithm draws $Y_n\\sim p_n(\\cdot\\mid x_{1:n-1})$ and outputs $X_n\\in\\mathcal V$ with marginal $q_n(\\cdot\\mid x_{1:n-1})$. Define $R_n\\equiv\\mathbf 1\\{X_n\\ne Y_n\\}$ and $N_{\\mathrm{rej}}\\equiv\\sum_{n=1}^T R_n$.\n\nStep 1 (Reduction to couplings at each step). For each $n$ and prefix $x\\equiv x_{1:n-1}$, the pair $(X_n,Y_n)$ under $A$ induces a coupling $K_n^A(\\cdot,\\cdot\\mid x)$ of $q_n(\\cdot\\mid x)$ and $p_n(\\cdot\\mid x)$. Conversely, any family of couplings $K_n(\\cdot,\\cdot\\mid x)$ can be realized by first sampling $Y_n\\sim p_n(\\cdot\\mid x)$ and then $X_n\\sim K_n(\\cdot\\mid Y_n,x)$, which preserves $X_n\\sim q_n(\\cdot\\mid x)$. Since unbiasedness enforces $X_{1:n-1}\\sim q$, expectations at step $n$ are taken w.r.t. $x\\sim q$ for every unbiased algorithm.\n\nStep 2 (Dynamic program). Let $\\varphi$ be nondecreasing on $\\{0,1,\\dots,T\\}$. For $n\\in\\{1,\\dots,T+1\\}$, $x\\in\\mathcal V^{n-1}$ and $s\\in\\{0,1,\\dots,n-1\\}$ define\n$$\nW_n(x,s)\\equiv\\inf_{\\text{valid one-proposal algorithms from step }n}\\ \\mathbb E\\big[\\,\\varphi\\big(s+\\textstyle\\sum_{j=n}^T R_j\\big)\\,\\big|\\,X_{1:n-1}=x\\big].\n$$\nSet $W_{T+1}(x,s)=\\varphi(s)$ for all $x,s$. For $n\\le T$ and any coupling $K$ of $q_n(\\cdot\\mid x)$ and $p_n(\\cdot\\mid x)$ we have\n$$\n\\mathbb E_{(u,v)\\sim K}\\big[\\,W_{n+1}(xu,\\ s+\\mathbf 1\\{u\\ne v\\})\\,\\big]\n= \\mathbb E_{u\\sim q_n(\\cdot\\mid x)}\\big[W_{n+1}(xu,s)\\big]\n+ \\mathbb E_{(u,v)\\sim K}\\big[\\Delta_{n+1}(xu;s)\\,\\mathbf 1\\{u\\ne v\\}\\big],\n$$\nwhere $\\Delta_{n+1}(xu;s)\\equiv W_{n+1}(xu,s+1)-W_{n+1}(xu,s)\\ge 0$, by backward induction from the assumption that $\\varphi$ is nondecreasing on $\\{0,\\dots,T\\}$. Hence, for fixed $(x,s)$, minimizing the Bellman expression is equivalent to maximizing\n$$\n\\sum_{u\\in\\mathcal V} \\Delta_{n+1}(xu;s)\\, K(u,u)\\quad\\text{over all couplings $K$ of $q_n(\\cdot\\mid x)$ and $p_n(\\cdot\\mid x)$}.\n$$\n\nStep 3 (Optimal stepwise coupling via a tight upper bound and explicit attainment). Fix $(x,s)$ and abbreviate $q\\equiv q_n(\\cdot\\mid x)$, $p\\equiv p_n(\\cdot\\mid x)$, and $\\Delta(u)\\equiv \\Delta_{n+1}(xu;s)\\ge 0$. For any coupling $K$ of $(q,p)$, each diagonal entry satisfies\n$$\nK(u,u)\\le \\min\\{q(u),p(u)\\}\\qquad(\\forall u\\in\\mathcal V),\n$$\nso, using nonnegativity of the weights,\n$$\n\\sum_{u}\\Delta(u)\\,K(u,u)\\ \\le\\ \\sum_{u}\\Delta(u)\\,\\min\\{q(u),p(u)\\}.\\tag{*}\n$$\nWe claim the upper bound (*) is attained. Define the overlap and residuals\n$$\n\\rho(u)\\equiv \\min\\{q(u),p(u)\\},\\qquad r(u)\\equiv [\\,q(u)-p(u)\\,]_+,\\qquad s(u)\\equiv [\\,p(u)-q(u)\\,]_+.\n$$\nThen $\\sum_u r(u)=\\sum_u s(u)=\\operatorname{TV}(q,p)$ and $r(u)\\,s(u)=0$ for every $u$. Choose any matrix $L$ supported on $\\{(i,j): r(i)>0,\\ s(j)>0\\}$ with row sums $r(i)$ and column sums $s(j)$. Now define a coupling $K^*$ by\n$$\nK^*(u,u)=\\rho(u)\\quad(\\forall u),\\qquad K^*(i,j)=L(i,j)\\ \\text{ for }\\ i\\in\\{r>0\\},\\ j\\in\\{s>0\\},\n$$\nand $K^*(i,j)=0$ otherwise. Then $K^*$ has marginals $(q,p)$, achieves $K^*(u,u)=\\min\\{q(u),p(u)\\}$ for all $u$, and hence attains equality in (*). Consequently, for the Bellman step at $(x,s)$, any maximal-agreement coupling (one with $K(u,u)=\\min\\{q(u),p(u)\\}$ for all $u$) is optimal. In particular, this coupling agrees with the standard token-level SD rule: with probability $\\sum_u \\min\\{q(u),p(u)\\}=1-\\operatorname{TV}(q,p)$ it sets $X=Y$, and otherwise draws $X$ from the residual $[q-p]_+/\\operatorname{TV}(q,p)$.\n\nBackward induction (sufficiency). Since at every $(x,s)$ a maximal-agreement coupling is optimal for the Bellman step, applying it at each $n=1,\\dots,T$ yields a globally optimal policy. Hence\n$$\n\\mathbb E_{\\mathrm{SD}}\\big[\\varphi(N_{\\mathrm{rej}})\\big]=\\mathbb E\\big[W_1(\\emptyset,0)\\big]\\ \\le\\ \\mathbb E_A\\big[\\varphi(N_{\\mathrm{rej}})\\big]\n$$\nfor every unbiased one-proposal algorithm $A$ with proposal conditionals $(p_n)$. Equivalently, $N_{\\mathrm{rej}}^{\\mathrm{SD}}$ is minimal in the increasing (stochastic) order among all such algorithms.\n\nNecessity of maximal agreement for mean-optimality. Take $\\varphi(t)=t$. Then $\\Delta_{n+1}(xu;s)\\equiv 1$ for all states, so the Bellman step reduces to maximizing $\\sum_u K(u,u)$ subject to $K(u,u)\\le\\min\\{q(u),p(u)\\}$, which forces $K(u,u)=\\min\\{q(u),p(u)\\}$ for every $u$. Thus any algorithm that minimizes $\\mathbb E[N_{\\mathrm{rej}}]$ must, at every step and prefix, use a maximal-agreement coupling; in particular, any algorithm that is simultaneously optimal for all nondecreasing $\\varphi$ coincides with SD on the agreement event and attains $\\mathbb P\\{X_n=Y_n\\mid x\\}=1-\\operatorname{TV}(q_n(\\cdot\\mid x),p_n(\\cdot\\mid x))$ at each step.\n\nConsequences. Because the inequality holds for every nondecreasing $\\varphi$ on $\\{0,\\dots,T\\}$:\n- With $\\varphi(t)=t$, SD minimizes $\\mathbb E[N_{\\mathrm{rej}}]$ and attains the instance-dependent lower bound $\\sum_{n=1}^T\\mathbb E_{x\\sim q}[\\operatorname{TV}(p_n(\\cdot\\mid x),q_n(\\cdot\\mid x))]$.\n- With $\\varphi(t)=t^2$, SD minimizes the second moment $\\mathbb E[N_{\\mathrm{rej}}^2]$. Consequently, within any class of unbiased algorithms having the same mean $\\mathbb E[N_{\\mathrm{rej}}]$, SD minimizes the variance.\n- With $\\varphi(t)=e^{\\lambda t}$ for $\\lambda>0$, SD minimizes all exponential moments of $N_{\\mathrm{rej}}$.\n\nThus, among all unbiased token-level draft–verify algorithms using exactly one proposal per position with the same conditionals $(p_n)$, standard SD minimizes $\\mathbb E[\\varphi(N_{\\mathrm{rej}})]$ for every nondecreasing $\\varphi$ on $\\{0,\\dots,T\\}$, establishing increasing-order optimality and characterizing stepwise optimality via maximal-agreement couplings. $\\square$\n\\end{proof}"
  },
  {
    "statement": "Suppose at each step the verifier uses a coupling whose agreement probability is $1-\\operatorname{TV}(p_n,q_n)-\\epsilon_n$ with $\\epsilon_n\\in[0,1-\\operatorname{TV}(p_n,q_n)]$, and employs a single-step rejection–correction that, upon spurious rejection, resamples from a calibrated correction kernel preserving the marginal $q_n$. Then the overall joint law remains exact, and the expected extra rejections incurred over SD satisfy $$\\mathbb E[\\Delta N_{\\mathrm{rej}}]=\\sum_{n=1}^T\\mathbb E[\\epsilon_n].$$",
    "proof_markdown": "Let $T<\\infty$ and, for each step $n$ and prefix $x_{1:n-1}$, let $p_n(\\cdot\\mid x_{1:n-1})$ and $q_n(\\cdot\\mid x_{1:n-1})$ be distributions on a common measurable space. Suppose the verifier uses a coupling at step $n$ whose agreement probability equals $1-\\operatorname{TV}(p_n,q_n)-\\epsilon_n$ with $\\epsilon_n\\in[0,1-\\operatorname{TV}(p_n,q_n)]$, and upon a spurious rejection performs a single-step rejection–correction that resamples from a calibrated kernel preserving the marginal $q_n(\\cdot\\mid x_{1:n-1})$.\n\nFix a step $n$ and a prefix $x\\equiv x_{1:n-1}$. For brevity write $p:=p_n(\\cdot\\mid x)$ and $q:=q_n(\\cdot\\mid x)$, and set $t:=\\operatorname{TV}(p,q)$. Let\n$$\n\\begin{aligned}\n&c \\equiv p\\wedge q,\\qquad r \\equiv [q-p]_+,\\qquad s \\equiv [p-q]_+,\\\\\n&\\|c\\|_1=1-t,\\quad \\|r\\|_1=\\|s\\|_1=t,\\quad q=c+r,\\quad p=c+s.\n\\end{aligned}\n$$\nBy assumption, the verifier employs a coupling $\\pi$ of $(p,q)$ whose on-diagonal mass is $\\mathbb P_{\\pi}\\{Y=Z\\}=1-t-\\epsilon$ for some $\\epsilon\\in[0,1-t]$. Let the on-diagonal measure be\n$$\n\\alpha(v)\\equiv \\pi\\{Y=Z=v\\},\\qquad v\\in\\mathcal V.\n$$\nThen $0\\le \\alpha\\le c$ (as measures) and $\\|\\alpha\\|_1=1-t-\\epsilon$. Define the spurious shortfall measure $\\sigma:=c-\\alpha\\ge 0$, so $\\|\\sigma\\|_1=\\epsilon$.\n\nSingle-step rejection–correction. Condition on the coupling’s outcome:\n- Accept if $Y=Z$, and set $X_n:=Y$; the unconditional contribution to $\\mathcal L(X_n\\mid x)$ is the measure $\\alpha$.\n- Otherwise, a rejection occurs. We distinguish (measurably within the coupling) between (i) true mismatches of total mass $t$, and (ii) spurious rejections of total mass $\\epsilon$ (the withheld common mass $\\sigma$). On rejection we resample $X_n$ from the calibrated kernel that draws\n$$X_n\\sim r/t\\quad\\text{for true mismatches (when }t>0\\text{)},\\qquad X_n\\sim \\sigma/\\epsilon\\quad\\text{for spurious rejections (when }\\epsilon>0\\text{)}.$$\nThus the unconditional contribution from all rejections to $\\mathcal L(X_n\\mid x)$ is\n$$\n t\\cdot \\frac{r}{t} + \\epsilon\\cdot \\frac{\\sigma}{\\epsilon} = r+\\sigma.\n$$\nHence the final conditional law at step $n$ is\n$$\n\\mathcal L(X_n\\mid x)\\ =\\ \\alpha + (r+\\sigma)\\ =\\ (\\alpha+\\sigma)+r\\ =\\ c+r\\ =\\ q.\n$$\nSince this holds for every prefix $x$, we have for all $x_{1:T}$ by the chain rule\n$$\n\\mathbb P\\{X_{1:T}=x_{1:T}\\}\\ =\\ \\prod_{n=1}^T q_n(x_n\\mid x_{1:n-1})\\ =\\ q(x_{1:T}),\n$$\nso the joint law is exact.\n\nExtra rejections. In standard SD (maximal agreement), the rejection probability at step $n$ given $x$ equals $t=\\operatorname{TV}(p,q)$. Under the approximate coupling it is $t+\\epsilon$. Therefore the per-step excess rejection probability equals $\\epsilon$, and summing over steps and averaging over random prefixes yields\n$$\n\\mathbb E[\\Delta N_{\\mathrm{rej}}]\\ =\\ \\sum_{n=1}^T \\mathbb E[\\epsilon_n].\n$$\nThis also covers the boundary cases: if $1-t=0$ then necessarily $\\epsilon=0$; if $t=0$ (resp. $\\epsilon=0$) the corresponding rejection branch is vacuous."
  },
  {
    "statement": "(Block full-acceptance lower bound via additive TVs) For K-block draft–verify with longest-prefix verification and fixed proposal conditionals p_{n:n+K-1}, let \\(\\pi_{n+i}:=\\operatorname{TV}(p_{n+i}(\\cdot\\mid X_{1:n+i-1}),q_{n+i}(\\cdot\\mid X_{1:n+i-1}))\\). Then the full-acceptance probability satisfies the pathwise inequality \\(\\prod_{i=0}^{K-1}(1-\\pi_{n+i})\\ge 1-\\sum_{i=0}^{K-1}\\pi_{n+i}\\). Averaging over prefixes yields the unconditional bound \n\\[\\mathbb{P}\\{A^{(K)}=K\\}\\;\\ge\\; 1-\\sum_{i=0}^{K-1}\\mathbb{E}[\\operatorname{TV}(p_{n+i},q_{n+i})].\\]",
    "proof_markdown": "Proof. Fix a block starting at position $n$ of length $K$, and the longest-prefix verifier with proposal conditionals $(p_{n},\\dots,p_{n+K-1})$. For each $i\\in\\{0,\\dots,K-1\\}$ define the prefix-measurable total-variation gap\n\\[\n\\pi_{n+i} \\,=\\, \\operatorname{TV}\\big(p_{n+i}(\\cdot\\mid X_{1:n+i-1}),\\, q_{n+i}(\\cdot\\mid X_{1:n+i-1})\\big)\\in[0,1].\n\\]\nBy the maximal coupling lemma, at step $n+i$ and conditional on the event that the first $i$ tokens in the block have been accepted (so both models condition on the same realized prefix $X_{1:n+i-1}$), there exists a coupling such that\n\\[\n\\mathbb{P}\\{\\text{token }n+i\\text{ is accepted}\\mid X_{1:n+i-1},\\, A^{(i)}=i\\}\\;=\\;1-\\pi_{n+i}.\n\\]\nConstruct the joint coupling sequentially via the standard overlap–residual procedure, using fresh independent auxiliary randomness at each step. Then, by the chain rule of conditional probabilities,\n\\[\n\\mathbb{P}\\{A^{(K)}=K\\mid X_{1:n+K-1}\\}\\;=\\;\\prod_{i=0}^{K-1}\\big(1-\\pi_{n+i}\\big).\n\\]\nWe now use the elementary inequality: for any $a_0,\\dots,a_{K-1}\\in[0,1]$,\n\\[\n\\prod_{i=0}^{K-1}(1-a_i)\\;\\ge\\;1-\\sum_{i=0}^{K-1}a_i.\n\\]\nA short induction proves it: for $K=1$ it is equality; if it holds for $K-1$, then\n\\[\n\\prod_{i=0}^{K-1}(1-a_i) - \\Big(1-\\sum_{i=0}^{K-1}a_i\\Big)\n= \\Big[\\prod_{i=0}^{K-2}(1-a_i) - \\Big(1-\\sum_{i=0}^{K-2}a_i\\Big)\\Big] \n\\; +\\; a_{K-1}\\Big(1-\\prod_{i=0}^{K-2}(1-a_i)\\Big)\\;\\ge\\;0.\n\\]\nApplying this pathwise with $a_i=\\pi_{n+i}$ yields\n\\[\n\\mathbb{P}\\{A^{(K)}=K\\mid X_{1:n+K-1}\\}\\;=\\;\\prod_{i=0}^{K-1}(1-\\pi_{n+i})\\;\\ge\\;1-\\sum_{i=0}^{K-1}\\pi_{n+i}.\n\\]\nFinally, taking expectations over the (target) prefix randomness and the internal coins gives\n\\[\n\\mathbb{P}\\{A^{(K)}=K\\}\\;=\\;\\mathbb{E}\\Big[\\prod_{i=0}^{K-1}(1-\\pi_{n+i})\\Big]\n\\;\\ge\\;\\mathbb{E}\\Big[1-\\sum_{i=0}^{K-1}\\pi_{n+i}\\Big]\n\\;=\\;1-\\sum_{i=0}^{K-1}\\mathbb{E}\\big[\\operatorname{TV}(p_{n+i},q_{n+i})\\big],\n\\]\nwhere $\\mathbb{E}[\\operatorname{TV}(p_{n+i},q_{n+i})]$ abbreviates $\\mathbb{E}_{X_{1:n+i-1}\\sim q}\\big[\\operatorname{TV}(p_{n+i}(\\cdot\\mid X_{1:n+i-1}),q_{n+i}(\\cdot\\mid X_{1:n+i-1}))\\big]$. This proves the stated bound. ∎"
  },
  {
    "statement": "Fix a block size K and proposal conditionals p_{n:n+K-1}. Among all unbiased K-block draft–verify schemes using these proposals, the longest-prefix verifier produces an accepted-token count A^{(K)} that dominates that of any other such scheme in the increasing convex order; equivalently, E[φ(A^{(K)})] is maximized simultaneously for every increasing convex φ:{0,…,K}→ℝ. Moreover, writing B^{(K)}:=K−A^{(K)}, the rejection count is minimal in the increasing convex order; in particular, within any subclass of schemes having the same mean rejections, B^{(K)} is minimal in the convex order.",
    "proof_markdown": "Claim. Fix a block size $K$ and proposal conditionals $p_{n:n+K-1}$. Let $A\\equiv A^{(K)}$ denote the accepted-token count under the longest-prefix (LP) verifier, and let $\\tilde A\\equiv \\tilde A^{(K)}$ be the accepted-token count under any other unbiased $K$-block draft–verify scheme using the same proposals. Then\n\n1) $\\mathbb{E}[\\phi(A)]\\ge \\mathbb{E}[\\phi(\\tilde A)]$ for every increasing convex $\\phi:\\{0,\\dots,K\\}\\to\\mathbb{R}$, i.e., $A\\succeq_{\\mathrm{icx}}\\tilde A$.\n\n2) Writing rejections within the block as $B:=K-A$ and $\\tilde B:=K-\\tilde A$, one has $B\\preceq_{\\mathrm{icx}}\\tilde B$, i.e., $\\mathbb{E}[h(B)]\\le \\mathbb{E}[h(\\tilde B)]$ for every increasing convex $h:\\{0,\\dots,K\\}\\to\\mathbb{R}$. In particular, within any subclass of verifiers having the same mean rejections $\\mathbb{E}[B]=\\mathbb{E}[\\tilde B]$, this implies the (mean-preserving) convex-order minimality $B\\preceq_{\\mathrm{cx}}\\tilde B$.\n\nProof. By the stated block-level dominance fact (context), for every $j\\in\\{0,\\dots,K\\}$,\n$$\n\\mathbb{P}_{\\mathrm{LP}}\\{A\\ge j\\}\\ \\ge\\ \\mathbb{P}_{\\text{any such scheme}}\\{\\tilde A\\ge j\\}.\n$$\nThat is, $A$ first-order stochastically dominates $\\tilde A$ ($A\\succeq_{\\mathrm{st}}\\tilde A$).\n\nWe use the discrete tail-sum representation: for any function $g:\\{0,\\dots,K\\}\\to\\mathbb{R}$ and any $X\\in\\{0,\\dots,K\\}$,\n$$\n\\mathbb{E}[g(X)]\\;=\\; g(0)+\\sum_{i=1}^K \\Delta g(i)\\,\\mathbb{P}\\{X\\ge i\\},\\qquad \\Delta g(i):=g(i)-g(i-1),\n$$\nwhich follows from $g(X)-g(0)=\\sum_{i=1}^X\\Delta g(i)$ and taking expectations.\n\n1) Let $\\phi$ be increasing and convex. Then $\\Delta\\phi(i)\\ge 0$. Using the tail representation and $\\mathbb{P}\\{A\\ge i\\}\\ge \\mathbb{P}\\{\\tilde A\\ge i\\}$ termwise,\n$$\n\\mathbb{E}[\\phi(A)]-\\phi(0)\n=\\sum_{i=1}^K \\Delta\\phi(i)\\,\\mathbb{P}\\{A\\ge i\\}\n\\ \\ge\\ \\sum_{i=1}^K \\Delta\\phi(i)\\,\\mathbb{P}\\{\\tilde A\\ge i\\}\n=\\mathbb{E}[\\phi(\\tilde A)]-\\phi(0),\n$$\nproving $A\\succeq_{\\mathrm{icx}}\\tilde A$.\n\n2) Put $B:=K-A$ and $\\tilde B:=K-\\tilde A$. From $A\\succeq_{\\mathrm{st}}\\tilde A$ we get $B\\preceq_{\\mathrm{st}}\\tilde B$ (reverse order under the decreasing affine map $b\\mapsto K-b$). Let $h$ be increasing convex on $\\{0,\\dots,K\\}$. Then $\\Delta h(i)\\ge 0$ for all $i$. Using the tail representation and $\\mathbb{P}\\{B\\ge i\\}\\le\\mathbb{P}\\{\\tilde B\\ge i\\}$ termwise,\n$$\n\\mathbb{E}[h(B)]-h(0)\n=\\sum_{i=1}^K \\Delta h(i)\\,\\mathbb{P}\\{B\\ge i\\}\n\\ \\le\\ \\sum_{i=1}^K \\Delta h(i)\\,\\mathbb{P}\\{\\tilde B\\ge i\\}\n=\\mathbb{E}[h(\\tilde B)]-h(0),\n$$\nwhich yields $\\mathbb{E}[h(B)]\\le \\mathbb{E}[h(\\tilde B)]$ for all increasing convex $h$, i.e., $B\\preceq_{\\mathrm{icx}}\\tilde B$.\n\nFinally, if two schemes have the same mean rejections $\\mathbb{E}[B]=\\mathbb{E}[\\tilde B]$, then the above $\\mathrm{icx}$ inequality extends to all convex $h$ by the standard affine-tilt normalization: for a convex $h$, set $c=\\Delta h(1)$ and $\\psi(b):=h(b)-c b$, which is increasing convex; applying the $\\mathrm{icx}$ inequality to $\\psi$ and using $\\mathbb{E}[B]=\\mathbb{E}[\\tilde B]$ gives $\\mathbb{E}[h(B)]\\le \\mathbb{E}[h(\\tilde B)]$, i.e., $B\\preceq_{\\mathrm{cx}}\\tilde B$ in that iso-mean subclass. ∎"
  },
  {
    "statement": "(Mixture-of-temperatures schedule with bounded cumulative rejection) For a temperature schedule \\(\\{\\tau_t\\}_{t=1}^L\\subset[1-\\rho,1+\\rho]\\) and a successive draft–verify schedule that at micro-step \\(t\\) proposes from \\(p_{\\tau_t}\\), the total expected number \\(R\\) of rejections across the \\(L\\) micro-steps at a fixed position satisfies\n\n\\[\\mathbb{E}[R]\\ \\le\\ \\frac{1}{2}\\,\\mathbb{E}_{q_n}\\Big[\\sum_{t=1}^L |\\tau_t-1|\\,\\big|\\log q_n(V)-\\mathbb{E}_{q_n}[\\log q_n(V)]\\big|\\Big]+C(m,\\rho)\\sum_{t=1}^L(\\tau_t-1)^2,\\]\n\nwith the same uniform constant \\(C(m,\\rho)\\) as in the two-sided local law; hence path length in the \\(\\tau\\)-metric given by the log-probability MAD controls cumulative rejection.",
    "proof_markdown": "Proof. Fix a position $n$ and a prefix $x_{1:n-1}$. Let $q\\equiv q_n(\\cdot\\mid x_{1:n-1})$ be supported on $S$ with $|S|\\le m$, and for each $t\\in\\{1,\\dots,L\\}$ with $\\tau_t\\in[1-\\rho,1+\\rho]$ define the temperature-scaled proposal\n$$\n p_{\\tau_t}(v)=\\frac{q(v)^{\\tau_t}}{\\sum_{u\\in S}q(u)^{\\tau_t}},\\qquad v\\in S.\n$$\nBy the (pointwise, uniform) local temperature-sensitivity law, there exists a constant $C(m,\\rho)$ (independent of $n$, the prefix, and $t$) such that, writing\n$$\n\\mathrm{MAD}(q):=\\mathbb{E}_{q}\\big[\\,\\big|\\log q(V)-\\mathbb{E}_{q}[\\log q(V)]\\big|\\,\\big],\n$$\nwe have, for every $t$,\n$$\n\\operatorname{TV}(p_{\\tau_t},q)\n\\;\\le\\; \\frac{|\\tau_t-1|}{2}\\,\\mathrm{MAD}(q)\\; +\\; C(m,\\rho)\\,(\\tau_t-1)^2.\\tag{1}\n$$\nSumming (1) over $t=1,\\dots,L$ gives, pointwise in the prefix,\n$$\n\\sum_{t=1}^L \\operatorname{TV}(p_{\\tau_t},q)\n\\;\\le\\; \\frac{1}{2}\\,\\mathrm{MAD}(q)\\sum_{t=1}^L |\\tau_t-1|\\; +\\; C(m,\\rho)\\sum_{t=1}^L (\\tau_t-1)^2.\\tag{2}\n$$\nTaking the expectation over prefixes and recalling that $\\mathrm{MAD}(q)=\\mathbb{E}_{q_n}[\\,|\\log q_n(V)-\\mathbb{E}_{q_n}[\\log q_n(V)]|\\,]$, we obtain\n$$\n\\sum_{t=1}^L \\mathbb{E}\\big[\\operatorname{TV}(p_{\\tau_t,n},q_n)\\big]\n\\;\\le\\; \\frac{1}{2}\\,\\mathbb{E}_{q_n}\\!\\Big[\\sum_{t=1}^L |\\tau_t-1|\\,\\big|\\log q_n(V)-\\mathbb{E}_{q_n}[\\log q_n(V)]\\big|\\Big]\n\\; +\\; C(m,\\rho)\\sum_{t=1}^L (\\tau_t-1)^2.\\tag{3}\n$$\nTo connect (3) to cumulative rejections for a successive draft–verify schedule at this fixed position, we use a coupling that avoids conditioning on execution. Fix the prefix and let $X\\sim q$ be the target token. For each $t$, define the acceptance function on the support of $q$ by\n$$\n\\alpha_t(v):=\\min\\Big\\{1,\\frac{p_{\\tau_t}(v)}{q(v)}\\Big\\},\\qquad v\\in S,\n$$\nand the residual proposal\n$$\n r_t(v):=\\frac{\\big[p_{\\tau_t}(v)-\\alpha_t(v)\\,q(v)\\big]_+}{\\operatorname{TV}(p_{\\tau_t},q)},\\qquad v\\in S.\n$$\nNow construct, conditionally independently across $t$ given $X$, Bernoulli coins $B_t\\sim\\operatorname{Bern}(\\alpha_t(X))$ and, when $B_t=0$, draw $Z_t\\sim r_t$ independently of everything else; set $Y_t:=X$ if $B_t=1$ and $Y_t:=Z_t$ if $B_t=0$. Then for each $t$ the pair $(X,Y_t)$ has marginals $(q,p_{\\tau_t})$ and is a maximal-agreement coupling, so\n$$\n\\mathbb{P}\\{Y_t=X\\}=\\sum_{v\\in S}\\min\\{q(v),p_{\\tau_t}(v)\\}=1-\\operatorname{TV}(p_{\\tau_t},q),\\qquad\n\\mathbb{E}[\\mathbf{1}\\{Y_t\\ne X\\}]=\\operatorname{TV}(p_{\\tau_t},q).\\tag{4}\n$$\nRun the successive draft–verify schedule using the proposals $(Y_t)_{t\\le L}$ and stopping at the first index $\\tau^*\\le L$ for which $Y_{\\tau^*}=X$. Let $R$ be the number of rejections across these $L$ micro-steps. Pathwise we have\n$$\nR\\;=\\;\\#\\{t<\\tau^*: B_t=0\\}\\;\\le\\;\\sum_{t=1}^L \\mathbf{1}\\{B_t=0\\}.\n$$\nTaking expectation conditional on the prefix and using (4),\n$$\n\\mathbb{E}[R\\mid x_{1:n-1}]\\;\\le\\;\\sum_{t=1}^L \\mathbb{E}[1-B_t\\mid x_{1:n-1}]\\;=\\;\\sum_{t=1}^L \\operatorname{TV}(p_{\\tau_t},q).\n$$\nAveraging over prefixes yields the unconditional bound\n$$\n\\mathbb{E}[R]\\;\\le\\;\\sum_{t=1}^L \\mathbb{E}\\big[\\operatorname{TV}(p_{\\tau_t,n},q_n)\\big].\\tag{5}\n$$\nCombining (5) with (3) gives\n$$\n\\mathbb{E}[R]\\ \\le\\ \\frac{1}{2}\\,\\mathbb{E}_{q_n}\\Big[\\sum_{t=1}^L |\\tau_t-1|\\,\\big|\\log q_n(V)-\\mathbb{E}_{q_n}[\\log q_n(V)]\\big|\\Big]\\ +\\ C(m,\\rho)\\sum_{t=1}^L(\\tau_t-1)^2,\n$$\nwhich is the claimed bound on cumulative expected rejections. ∎"
  },
  {
    "statement": "(Two-sided second-order temperature law with a single explicit constant) Let m∈ℕ and ρ∈(0,1). If each conditional q_n(·|x_{1:n−1}) has support size at most m and |τ−1|≤ρ, then uniformly over steps n and prefixes x_{1:n−1},\n\n$$\n\\Big|\\operatorname{TV}(p_{\\tau,n},q_n)-\\frac{|\\tau-1|}{2}\\,\\mathbb{E}_{q_n}\\big[\\,|\\log q_n(V)-\\mathbb{E}_{q_n}[\\log q_n(V)]|\\,\\big]\\Big|\\ \\le\\ C(m,\\rho)\\,(\\tau-1)^2,\n$$\n\nwhere\n$$\nC(m,\\rho)=(1+\\rho\\log m)\\,m^{\\rho}\\Big(m\\,B_1(\\rho)+ (\\log m)^2 m^{\\rho}\\Big),\\qquad B_1(\\rho)=\\frac{4}{e^2(1-\\rho)^2}.\n$$\nEquivalently, the two-sided bounds hold with the same constant: one may take $C_-(m,\\rho)=C_+(m,\\rho)=C(m,\\rho)$. Consequently, $\\sum_n\\mathbb{E}[\\operatorname{TV}(p_{\\tau,n},q_n)]$ has matching linear terms in $|\\tau-1|$ with a quadratic remainder bounded by $C(m,\\rho)$ per step, uniformly in $n$.",
    "proof_markdown": "Fix m∈ℕ and ρ∈(0,1). For any step n and prefix x_{1:n−1}, let q≡q_n(·|x_{1:n−1}) be supported on a set S with |S|≤m, and for |τ−1|≤ρ define\n$$\n p_{\\tau}(v)=\\frac{q(v)^{\\tau}}{\\sum_{u\\in S}q(u)^{\\tau}},\\qquad v\\in S.\n$$\nWrite t:=τ−1 with |t|≤ρ. Let V∼q, set X:=\\log q(V), μ:=\\mathbb{E}_q[X]=\\sum_v q(v)\\log q(v)∈[−\\log m,0], and X_c:=X−μ. Then\n$$\n p_t(v)=\\frac{q(v)^{1+t}}{\\sum_u q(u)^{1+t}}=q(v)\\,\\frac{e^{tX_c(v)}}{Z_t},\\qquad Z_t:=\\mathbb{E}_q\\big[e^{tX_c}\\big].\n$$\nHence $\\operatorname{TV}(p_t,q)=\\tfrac12\\sum_v|p_t(v)−q(v)|=\\tfrac12\\,\\mathbb{E}_q[\\,|e^{tX_c}/Z_t−1|\\,]$. Define\n$$\nY_t:=\\frac{e^{tX_c}}{Z_t},\\qquad R_t:=Y_t−1−tX_c,\n$$\nso that\n$$\n\\operatorname{TV}(p_t,q)=\\frac12\\,\\mathbb{E}_q\\big[\\,|tX_c+R_t|\\,\\big].\n$$\nWe will prove a uniform bound $\\mathbb{E}_q[|R_t|]\\le K(m,\\rho)\\,t^2$ with $K(m,\\rho)<\\infty$ depending only on $(m,\\rho)$. Then by the reverse triangle inequality,\n$$\n\\Big|\\mathbb{E}_q\\big[|tX_c+R_t|\\big]−|t|\\,\\mathbb{E}_q[|X_c|]\\Big|\\le \\mathbb{E}_q[|R_t|],\n$$\nyielding\n$$\n\\Big|\\operatorname{TV}(p_t,q)−\\tfrac{|t|}{2}\\,\\mathbb{E}_q[|X_c|]\\Big|\\le \\tfrac12\\,\\mathbb{E}_q[|R_t|]\\le \\tfrac12 K(m,\\rho)\\,t^2.\n$$\nThus the two-sided inequality holds with the same constant $C(m,\\rho)=\\tfrac12 K(m,\\rho)$, once we bound $K(m,\\rho)$ explicitly.\n\nFrom $R_t=e^{tX_c}/Z_t−1−tX_c$ we obtain\n$$\nR_t=\\frac{e^{tX_c}−1−tX_c}{Z_t}+\\Big(\\frac{1}{Z_t}−1\\Big)(1+tX_c)\n=\\frac{e^{tX_c}−1−tX_c}{Z_t}-\\frac{Z_t−1}{Z_t}-tX_c\\,\\frac{Z_t−1}{Z_t}.\n$$\nSince $e^y\\ge 1+y$ for all y, $e^{tX_c}−1−tX_c\\ge0$ a.s.; and by Jensen, $Z_t=\\mathbb{E}_q[e^{tX_c}]\\ge e^{t\\mathbb{E}_q[X_c]}=1$. Therefore,\n$$\n\\mathbb{E}_q[|R_t|]\\le \\frac{1}{Z_t}\\,\\mathbb{E}_q\\big[e^{tX_c}−1−tX_c\\big]+\\frac{Z_t−1}{Z_t}+|t|\\,\\mathbb{E}_q[|X_c|] \\cdot \\frac{Z_t−1}{Z_t}.\n$$\nLet $\\varepsilon:=\\mathrm{sgn}(t)$ and $A(t):=\\mathbb{E}_q\\big[e^{tX_c}−1−tX_c\\big]$. Using Taylor’s integral remainder,\n$$\nA(t)=\\int_0^{|t|}(|t|−s)\\,\\mathbb{E}_q\\big[X_c^2 e^{s\\varepsilon X_c}\\big]ds.\n$$\nBecause $\\mathbb{E}_q[X_c]=0$, we also have $Z_t−1=\\mathbb{E}_q[e^{tX_c}−1]=A(t)$. Hence\n$$\n\\mathbb{E}_q[|R_t|]\\le \\frac{1}{Z_t}\\big(2+|t|\\,\\mathbb{E}_q[|X_c|]\\big)A(t).\n$$\nBounding $Z_t\\ge1$ and $\\int_0^{|t|}(|t|−s)ds=\\tfrac{|t|^2}{2}$ gives\n$$\n\\mathbb{E}_q[|R_t|]\\le \\Big(1+\\tfrac{|t|}{2}\\,\\mathbb{E}_q[|X_c|]\\Big)|t|^2\\,\\sup_{0\\le u\\le |t|}\\mathbb{E}_q\\big[X_c^2 e^{u\\varepsilon X_c}\\big].\n$$\nNext, uniformly for $u\\in[0,\\rho]$,\n$$\n\\mathbb{E}_q\\big[X_c^2 e^{u\\varepsilon X_c}\\big]=e^{−u\\varepsilon\\mu}\\sum_{v\\in S} q(v)^{1+u\\varepsilon}\\big(\\log q(v)−\\mu\\big)^2.\n$$\nUsing $e^{|u\\mu|}\\le e^{\\rho|\\mu|}\\le m^{\\rho}$, $(a−b)^2\\le2a^2+2b^2$, and $\\sum_v q(v)^{1+u\\varepsilon}\\le m^{\\rho}$ (since for $\\alpha\\ge1$, $\\sum q^\\alpha\\le1\\le m^{\\rho}$, while for $\\alpha\\in(0,1)$, $\\sum q^\\alpha\\le m^{1-\\alpha}\\le m^{\\rho}$), we obtain\n$$\n\\mathbb{E}_q\\big[X_c^2 e^{u\\varepsilon X_c}\\big]\\le 2 m^{\\rho}\\Big(\\sum_{v} q(v)^{1+u\\varepsilon}\\log^2 q(v)\\Big)+2 m^{\\rho}\\mu^2\\!\\sum_{v} q(v)^{1+u\\varepsilon}.\n$$\nSince $1+u\\varepsilon\\ge 1−\\rho$ and, for $x\\in(0,1]$, the map $\\alpha\\mapsto x^\\alpha$ decreases in $\\alpha$, we have $x^{1+u\\varepsilon}\\le x^{1−\\rho}$. Thus\n$$\n\\sum_{v} q(v)^{1+u\\varepsilon}\\log^2 q(v)\\le |S|\\cdot \\sup_{x\\in(0,1]} x^{1−\\rho}(\\log x)^2\\le m\\,B_1(\\rho),\n$$\nwhere\n$$\nB_1(\\rho):=\\sup_{x\\in(0,1]} x^{1−\\rho}(\\log x)^2=\\sup_{y\\ge0} e^{−(1−\\rho)y}y^2=\\frac{4}{e^2(1−\\rho)^2}.\n$$\nUsing $|\\mu|\\le\\log m$ yields the uniform bound\n$$\n\\sup_{0\\le u\\le\\rho}\\mathbb{E}_q\\big[X_c^2 e^{u\\varepsilon X_c}\\big]\\le 2 m^{\\rho}\\big(m B_1(\\rho)+ (\\log m)^2 m^{\\rho}\\big)=:M(m,\\rho).\n$$\nFinally, since $X\\le0$ a.s., $\\mathbb{E}_q[|X|]=−\\mu\\le\\log m$, and hence $\\mathbb{E}_q[|X_c|]\\le \\mathbb{E}_q[|X|]+|\\mu|\\le 2\\log m$. Combining the pieces, for |t|≤ρ,\n$$\n\\mathbb{E}_q[|R_t|]\\le \\Big(1+\\tfrac{|t|}{2}\\,\\mathbb{E}_q[|X_c|]\\Big)|t|^2 M(m,\\rho)\\le (1+\\rho\\log m)\\,M(m,\\rho)\\,t^2.\n$$\nTherefore\n$$\n\\Big|\\operatorname{TV}(p_t,q)−\\frac{|t|}{2}\\,\\mathbb{E}_q[|X_c|]\\Big|\\le \\frac12\\,\\mathbb{E}_q[|R_t|]\\le \\frac{(1+\\rho\\log m)\\,M(m,\\rho)}{2}\\,t^2.\n$$\nSetting\n$$\nC(m,\\rho):=\\frac{(1+\\rho\\log m)\\,M(m,\\rho)}{2}=(1+\\rho\\log m)\\,m^{\\rho}\\big(m B_1(\\rho)+ (\\log m)^2 m^{\\rho}\\big)\n$$\nproves the claimed absolute deviation bound, hence the two-sided inequalities with the same constant $C(m,\\rho)$, uniformly over steps and prefixes. Averaging over prefixes and summing over n preserves the linear term and contributes at most the stated quadratic error $C(m,\\rho)(\\tau-1)^2$ per step. ∎"
  },
  {
    "statement": "For conditionals {q_n} satisfying \\(\\operatorname{TV}(q_{n+1}(\\cdot\\mid x_{1:n}),q_{n+1}(\\cdot\\mid x'_{1:n}))\\le \\gamma\\,\\mathbf{1}\\{x_n\\neq x'_n\\}\\) for some \\(\\gamma\\in[0,1)\\), employing a Markovian maximal coupling across \\(K\\)-blocks yields\n\\[\\mathbb{E}[A^{(K)}]\\ \\ge\\ \\sum_{j=0}^{K-1}\\prod_{i=0}^{j}(1-\\mathbb{E}[\\operatorname{TV}(p_{n+i},q_{n+i})]-\\gamma),\\]\nwith an explicit gain over tokenwise coupling whenever \\(\\gamma<\\min_i(1-\\mathbb{E}[\\operatorname{TV}(p_{n+i},q_{n+i})])\\).",
    "proof_markdown": "Fix \\(n\\) and \\(K\\). Assume\n\\[\\forall x_{1:n},x'_{1:n}:\\n\\operatorname{TV}\\big(q_{n+1}(\\cdot\\mid x_{1:n}),\\,q_{n+1}(\\cdot\\mid x'_{1:n})\\big)\\le \\gamma\\,\\mathbf{1}\\{x_n\\ne x'_n\\},\\qquad \\gamma\\in[0,1).\\]\nWork on the state space of prefixes: set \\(W_0:=X_{1:n-1}\\) and, for \\(i\\ge0\\), let \\(W_{i+1}:=(W_i,X_{n+i})\\) where \\(X_{n+i}\\sim q_{n+i}(\\cdot\\mid W_i)\\). Denote by \\(\\mathsf Q_i(w,\\cdot):=q_{n+i}(\\cdot\\mid w)\\) the time-\\(i\\) transition on prefixes. By the assumption, if \\(w,w'\\) share the same last token then \\(\\mathsf Q_i(w,\\cdot)=\\mathsf Q_i(w',\\cdot)\\); otherwise \\(\\operatorname{TV}(\\mathsf Q_i(w,\\cdot),\\mathsf Q_i(w',\\cdot))\\le\\gamma\\). Hence the Dobrushin coefficient satisfies\n\\[\\delta(\\mathsf Q_i):=\\sup_{w,w'}\\operatorname{TV}\\big(\\mathsf Q_i(w,\\cdot),\\mathsf Q_i(w',\\cdot)\\big)\\le\\gamma.\\]\nFor each in-block index \\(i\\in\\{0,\\dots,K-1\\}\\) and prefix \\(w\\), define the one-step maximal-agreement probability\n\\[U_i(w)\\ :=\\ 1-\\operatorname{TV}\\big(p_{n+i}(\\cdot\\mid w),\\ q_{n+i}(\\cdot\\mid w)\\big)\\in[0,1].\\]\nCouple the proposal and target processes by a Markovian maximal coupling across the block: at each step \\(i\\), conditionally on the current coupled prefix states, draw the next pair of tokens by a maximal coupling of the corresponding conditionals.\n\nLet \\(B_i\\) be the indicator that the \\(i\\)-th in-block tokens (position \\(n+i\\)) of the coupled proposal and target coincide, and set \\(T_i:=\\prod_{t=0}^{i} B_t\\in\\{0,1\\}\\) with the convention \\(T_{-1}\\equiv 1\\). The event that at least \\(j{+}1\\) tokens are accepted is \\(\\{T_j=1\\}\\), so\n\\[\\mathbb{P}\\{A^{(K)}\\ge j{+}1\\}\\ =\\ \\mathbb{E}[T_j],\\qquad j=0,1,\\dots,K-1.\\]\nCrucially, we have the one-step recursion\n\\[\\mathbb{E}[T_i]\\ =\\ \\mathbb{E}[\\,T_{i-1}\\,U_i(W_i)\\,],\\qquad i\\ge0.\\tag{1}\\]\nIndeed, conditionally on the entire past up to time \\(i\\) and on \\(W_i\\), the Markovian maximal coupling ensures \\(\\mathbb{P}\\{B_i=1\\mid T_{i-1}=1,\\,W_i\\}=U_i(W_i)\\), while \\(T_{i-1}=0\\) forces \\(T_i=0\\). Taking expectations yields (1).\n\nWe now lower bound \\(\\mathbb{E}[T_i]\\) using a one-step decoupling inequality for Markov chains.\n\nLemma (one-step \\(\\gamma\\)-decoupling). Let \\((W_i)\\) be a (possibly time-inhomogeneous) Markov chain with transition \\(\\mathsf Q_i\\) satisfying \\(\\delta(\\mathsf Q_i)\\le\\gamma\\). For any \\(i\\ge1\\), any \\(\\mathcal F_{i-1}\\)-measurable \\(Z\\in[0,1]\\) (where \\(\\mathcal F_{i-1}:=\\sigma(W_0,\\dots,W_{i-1})\\)), and any \\(f:\\text{state}\\to[0,1]\\),\n\\[\\mathbb{E}[\\,Z\\,f(W_i)\\,]\\ \\ge\\ \\mathbb{E}[Z]\\\\,\\big(\\mathbb{E}[f(W_i)]-\\gamma\\big).\\tag{2}\\]\nProof of the lemma. Let \\(\\mu_i:=\\mathcal L(W_i)\\) be the unconditional law of \\(W_i\\). By convexity of TV, \\(\\sup_w \\operatorname{TV}(\\mathsf Q_i(w,\\cdot),\\mu_i)\\le\\delta(\\mathsf Q_i)\\le\\gamma\\). Hence, for every \\(f\\in[0,1]\\) and all \\(w\\),\n\\(\\mathbb{E}[f(W_i)\\mid W_{i-1}=w]\\ge \\mathbb{E}[f(W_i)]-\\gamma\\). Taking conditional expectation with respect to \\(\\mathcal F_{i-1}\\) and multiplying by any \\(\\mathcal F_{i-1}\\)-measurable \\(Z\\in[0,1]\\) gives (2). \\(\\square\\)\n\nReturn to (1). Since \\(T_{i-1}\\) is \\(\\mathcal F_{i-1}\\)-measurable, write \\(Z_i:=\\mathbb{E}[T_{i-1}\\mid\\mathcal F_{i-1}]=T_{i-1}\\in[0,1]\\). Then\n\\[\\mathbb{E}[T_i]\n=\\mathbb{E}[\\,T_{i-1}\\,U_i(W_i)\\,]\n=\\mathbb{E}\\big[\\,Z_i\\,\\mathbb{E}[U_i(W_i)\\mid\\mathcal F_{i-1}]\\big]\n=\\mathbb{E}[\\,Z_i\\,U_i(W_i)\\,],\\]\nwhere the last equality uses \\(Z_i\\) being \\(\\mathcal F_{i-1}\\)-measurable. Applying the lemma (2) with \\(f=U_i\\) yields\n\\[\\mathbb{E}[T_i]\\ \\ge\\ \\mathbb{E}[Z_i]\\\\,\\big(\\mathbb{E}[U_i(W_i)]-\\gamma\\big)\n=\\mathbb{E}[T_{i-1}]\\\\,\\big(\\mathbb{E}[U_i(W_i)]-\\gamma\\big).\\tag{3}\\]\nBy induction on \\(i\\) (and noting \\(\\mathbb{E}[T_0]=\\mathbb{E}[U_0(W_0)]\\ge \\mathbb{E}[U_0(W_0)]-\\gamma\\)), (3) gives\n\\[\\mathbb{E}[T_j]\\ \\ge\\ \\prod_{i=0}^{j}\\big(\\mathbb{E}[U_i(W_i)]-\\gamma\\big),\\qquad j=0,1,\\dots,K-1.\\]\nFinally, by definition of \\(U_i\\) and taking expectations under the target prefix law and dynamics,\n\\[\\mathbb{E}[U_i(W_i)]\n=1-\\mathbb{E}[\\operatorname{TV}(p_{n+i},q_{n+i})].\\]\nTherefore\n\\[\\mathbb{E}[A^{(K)}]\n=\\sum_{j=0}^{K-1}\\mathbb{P}\\{A^{(K)}\\ge j{+}1\\}\n=\\sum_{j=0}^{K-1}\\mathbb{E}[T_j]\n\\ \\ge\\ \\sum_{j=0}^{K-1}\\ \\prod_{i=0}^{j}\\Big(1-\\mathbb{E}[\\operatorname{TV}(p_{n+i},q_{n+i})]-\\gamma\\Big),\\]\nwhich is the claimed bound. Whenever \\(\\gamma<\\min_i\\big(1-\\mathbb{E}[\\operatorname{TV}(p_{n+i},q_{n+i})]\\big)\\) every factor is strictly positive, so the right-hand side is strictly larger than the tokenwise worst-case guarantee (which corresponds to the vacuous replacement \\(\\gamma\\mapsto 1\\), yielding zero)."
  }
]