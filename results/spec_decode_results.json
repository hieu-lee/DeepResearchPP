[
  {
    "statement": "Fix block size K and proposal conditionals (p_{n:n+K-1}). Among all unbiased K-block draft–verify schemes, the longest-prefix verifier first-order stochastically dominates any other valid scheme: for all j\\in\\{0,\\dots,K\\}, \\mathbb{P}_{\\mathrm{LP}}[A^{(K)}\\ge j] \\ge \\mathbb{P}_{\\text{any valid}}[A^{(K)}\\ge j]. In particular, it maximizes \\mathbb{E}[A^{(K)}].",
    "proof_markdown": "**Proof.** Fix an iteration index $n$ and a realized history $h\\equiv x_{1:n-1}$. For $\\tau\\in\\{1,\\dots,K\\}$ let\n$$\nP_p^{(\\tau)}(x_{1:\\tau}\\mid h) \\,=\\, \\prod_{i=1}^{\\tau} p_{n+i-1}(x_i\\mid h, x_{1:i-1}),\\qquad\nP_q^{(\\tau)}(x_{1:\\tau}\\mid h) \\,=\\, \\prod_{i=1}^{\\tau} q_{n+i-1}(x_i\\mid h, x_{1:i-1})\n$$\ndenote the $\\tau$-step path laws of the proposal and target, respectively. Consider any unbiased (valid) $K$-block draft–verify scheme. By exactness, its (eventual) output law for the next $K$ tokens given $h$ is $P_q^{(K)}(\\cdot\\mid h)$. Therefore the scheme induces a coupling $\\Gamma_h$ of two random blocks $(X_{1:K},Y_{1:K})\\in\\mathcal V^K\\times\\mathcal V^K$ with marginals $P_p^{(K)}(\\cdot\\mid h)$ and $P_q^{(K)}(\\cdot\\mid h)$, where $X_{1:K}$ is the drafted block and $Y_{1:K}$ is the scheme’s (eventual) output block.\n\nLet the first-disagreement time be\n$$\n\\sigma\\,:=\\,\\inf\\{j\\ge 1: X_j\\ne Y_j\\}\\ (\\inf\\emptyset:=\\infty),\\qquad L^{(K)}\\,:=\\,\\min\\{K,\\,\\sigma-1\\}.\n$$\nBy construction of the induced coupling (accepted tokens are committed as the prefix of the output), the accepted-prefix length $A^{(K)}$ of the scheme satisfies $A^{(K)}\\le L^{(K)}$ almost surely, with equality $A^{(K)}=L^{(K)}$ for the longest-prefix (LP) verifier.\n\nHence, for every $j\\in\\{1,\\dots,K\\}$ and realized $h$,\n$$\n\\mathbb P\\big(A^{(K)}\\ge j\\mid h\\big)\\ \\le\\ \\Gamma_h\\big(X_{1:j}=Y_{1:j}\\big)\n\\ \\le\\ 1-\\operatorname{TV}\\big(P_p^{(j)}(\\cdot\\mid h),P_q^{(j)}(\\cdot\\mid h)\\big)\n\\ =\\ \\sum_{x_{1:j}\\in\\mathcal V^j} \\min\\Big\\{P_p^{(j)}(x_{1:j}\\mid h),\\,P_q^{(j)}(x_{1:j}\\mid h)\\Big\\}.\n\\tag{1}\n$$\nThe first inequality uses $\\{A^{(K)}\\ge j\\}\\subseteq\\{X_{1:j}=Y_{1:j}\\}$ under $\\Gamma_h$; the second is the maximal-coupling bound applied to the distributions on $\\mathcal V^j$.\n\nNow average (1) over the random prefix $h\\sim q$ (the target’s prefix law): for any valid scheme,\n$$\n\\mathbb P\\big(A^{(K)}\\ge j\\big)\n\\ \\le\\ \\mathbb E_{h\\sim q}\\Big[\\sum_{x_{1:j}} \\min\\{P_p^{(j)}(x_{1:j}\\mid h),P_q^{(j)}(x_{1:j}\\mid h)\\}\\Big]\n\\ =:\\ S_j.\n\\tag{2}\n$$\nSumming (2) over $j=1,\\dots,K$ and using $\\mathbb E[A^{(K)}]=\\sum_{j=1}^K \\mathbb P(A^{(K)}\\ge j)$ gives, for any valid scheme,\n$$\n\\mathbb E\\big[A^{(K)}\\big] \\ \\le\\ \\sum_{j=1}^K S_j.\n\\tag{3}\n$$\nFor the LP verifier, the block-verification optimality formula yields the exact value\n$$\n\\mathbb E\\big[A^{(K)}_{\\mathrm{LP}}\\big] \\ =\\ \\sum_{j=1}^K S_j.\n\\tag{4}\n$$\nSince each inequality in (2) is an individual upper bound with nonnegative slack and the sum of these slacks equals zero for LP by (3)–(4), every slack must be zero. Thus, for every $j\\in\\{1,\\dots,K\\}$,\n$$\n\\mathbb P_{\\mathrm{LP}}\\big(A^{(K)}\\ge j\\big) \\ =\\ S_j \\ \\ge\\ \\mathbb P_{\\text{any valid}}\\big(A^{(K)}\\ge j\\big).\n$$\nFor $j=0$ the inequality is trivial. Hence the LP verifier first-order stochastically dominates every other unbiased $K$-block scheme, and in particular maximizes $\\mathbb E[A^{(K)}]$. Equivalently, the LP-induced coupling attains the maximal agreement probabilities $\\Gamma_h(X_{1:j}=Y_{1:j})=1-\\operatorname{TV}(P_p^{(j)}(\\cdot\\mid h),P_q^{(j)}(\\cdot\\mid h))$ simultaneously for all $j$ (cf. Völlering, 2016). ∎"
  },
  {
    "statement": "Local temperature-sensitivity law (pointwise, uniform over bounded support). Fix m∈ℕ and any ρ∈(0,1). For each step n and prefix x_{1:n−1}, let q_n(·|x_{1:n−1}) be a distribution supported on a finite set S(n,x_{1:n−1}) with |S(n,x_{1:n−1})|≤m. For |τ−1|≤ρ, define the temperature-scaled proposal on this support by\n$$p_{\\tau,n}(v)=\\frac{q_n(v)^{\\tau}}{\\sum_{u\\in S(n,x_{1:n-1})} q_n(u)^{\\tau}}\\quad(v\\in S(n,x_{1:n-1})),\\qquad p_{\\tau,n}(v)=0\\ (v\\notin S(n,x_{1:n-1})).$$\nThen, uniformly over all n and prefixes,\n$$\\operatorname{TV}(p_{\\tau,n},q_n)=\\tfrac{|\\tau-1|}{2}\\,\\mathbb{E}_{q_n}\\big[\\,|\\log q_n(V)-\\mathbb{E}_{q_n}[\\log q_n(V)]|\\,\\big]+O\\big((\\tau-1)^2\\big),$$\nwhere the O-constant depends only on m and ρ (and not on n, the prefix, or q_n). Consequently, averaging over prefixes yields the per-step expected rejection\n$$\\tau_n(\\tau)=\\mathbb{E}_{x_{1:n-1}}\\big[\\operatorname{TV}(p_{\\tau,n},q_n)\\big]=\\tfrac{|\\tau-1|}{2}\\,\\mathbb{E}_{x_{1:n-1}}\\Big[\\mathbb{E}_{q_n}\\big[\\,|\\log q_n(V)-\\mathbb{E}_{q_n}[\\log q_n(V)]|\\,\\big]\\Big]+O\\big((\\tau-1)^2\\big),$$\nwith the same O-constant.",
    "proof_markdown": "Proof. Fix a step n and prefix x_{1:n-1}. Write q≡q_n(·|x_{1:n-1}), let S:=supp(q), and assume |S|≤m. Fix any ρ∈(0,1) and restrict to |τ−1|≤ρ so that τ>0. Define, for v∈S,\n$$p_τ(v)=\\frac{q(v)^τ}{Z(τ)},\\qquad Z(τ):=\\sum_{u\\in S} q(u)^τ,\\quad\\text{and set }p_τ(v)=0\\text{ for }v\\notin S.$$\nLet ℓ(v):=\\log q(v) for v∈S and s_τ:=\\mathbb E_{p_τ}[ℓ]. Then, for v∈S,\n$$\\frac{\\mathrm d}{\\mathrm dτ}\\log p_τ(v)=ℓ(v)-s_τ=:h_τ(v),\\qquad \\Rightarrow\\qquad \\dot p_τ(v)=p_τ(v)\\,h_τ(v).$$\nDifferentiating again and using \\(\\dot s_τ=\\sum_{u\\in S}ℓ(u)\\dot p_τ(u)=\\operatorname{Var}_{p_τ}(ℓ)\\) gives, for v∈S,\n$$\\ddot p_τ(v)=p_τ(v)\\big(h_τ(v)^2-\\operatorname{Var}_{p_τ}(ℓ)\\big). \\tag{1}$$\nFor v∉S we have p_τ(v)≡0 for τ>0, hence \\(\\dot p_τ(v)=\\ddot p_τ(v)=0\\).\n\nTaylor’s formula with integral remainder about τ=1 yields, for δ:=τ−1 and all v,\n$$p_{1+δ}(v)=q(v)+δ\\,\\dot p_1(v)+r_v(δ),\\qquad r_v(δ)=\\int_0^{δ}(δ-t)\\,\\ddot p_{1+t}(v)\\,\\mathrm dt.$$\nSumming absolute values and using \\(\\big||a+b|-|a|\\big|\\le|b|\\) gives\n$$\\Big|\\sum_v|p_{1+δ}(v)-q(v)|-|δ|\\sum_v|\\dot p_1(v)|\\Big|\\le\\sum_v|r_v(δ)|\\le\\int_0^{|δ|}(|δ|-t)\\sum_v|\\ddot p_{1+t}(v)|\\,\\mathrm dt. \\tag{2}$$\nFrom (1) on S and zeros off S,\n$$\\sum_v|\\ddot p_τ(v)|\\le\\sum_{v\\in S} p_τ(v)\\big(h_τ(v)^2+\\operatorname{Var}_{p_τ}(ℓ)\\big)=2\\operatorname{Var}_{p_τ}(ℓ)\\le2\\,\\mathbb E_{p_τ}[ℓ^2].$$\nFor |τ−1|≤ρ<1, write a:=τ∈[1−ρ,1+ρ]. Then\n$$\\mathbb E_{p_a}[ℓ^2]=\\frac{\\sum_{v\\in S} q(v)^a\\,ℓ(v)^2}{\\sum_{v\\in S} q(v)^a}.$$\nWith q(v)=e^{−s} and s≥0, we have \\(q(v)^a\\,ℓ(v)^2=e^{−a s}s^2\\le\\max_{s\\ge0}s^2e^{−a s}=4/(a^2e^2)\\le4/((1−ρ)^2e^2)\\). Hence\n$$\\sum_{v\\in S} q(v)^a\\,ℓ(v)^2\\le \\frac{4|S|}{(1-ρ)^2e^2}.$$\nMoreover, for a∈[1−ρ,1] we have \\(\\sum_{v\\in S} q(v)^a\\ge1\\), and for a∈[1,1+ρ], \\(\\sum_{v\\in S} q(v)^a\\ge |S|^{1-a}\\ge |S|^{−ρ}\\). Thus for all a∈[1−ρ,1+ρ],\n$$\\sum_{v\\in S} q(v)^a\\ge |S|^{−ρ}.$$\nTherefore,\n$$\\sup_{|τ-1|\\leρ}\\ \\sum_v|\\ddot p_τ(v)|\\ \\le\\ 2\\sup_{|τ-1|\\leρ}\\mathbb E_{p_τ}[ℓ^2]\\ \\le\\ \\frac{8}{e^2}\\,\\frac{|S|^{1+ρ}}{(1-ρ)^2}\\ \\le\\ \\frac{8}{e^2}\\,\\frac{m^{1+ρ}}{(1-ρ)^2}=:C_{m,ρ}. \\tag{3}$$\nCombining (2)–(3) yields \\(\\sum_v|r_v(δ)|\\le (C_{m,ρ}/2)\\,δ^2\\) for |δ|≤ρ. At τ=1, p_1=q and, for v∈S, \\(\\dot p_1(v)=q(v)\\big(ℓ(v)-\\mathbb E_q[ℓ]\\big)\\) (while \\(\\dot p_1(v)=0\\) for v∉S), hence\n$$\\sum_v|\\dot p_1(v)|=\\sum_{v\\in S} q(v)\\,\\big|ℓ(v)-\\mathbb E_q[ℓ]\\big|=\\mathbb E_q\\big[\\,|\\log q(V)-\\mathbb E_q[\\log q(V)]|\\,\\big].$$\nUsing \\(\\operatorname{TV}(p,q)=\\tfrac12\\sum_v|p-q|\\), we have, uniformly for |τ−1|≤ρ,\n$$\\operatorname{TV}(p_τ,q)=\\frac{|τ-1|}{2}\\,\\mathbb E_q\\big[\\,|\\log q(V)-\\mathbb E_q[\\log q(V)]|\\,\\big]+O\\big((τ-1)^2\\big),$$\nwhere the O-constant depends only on (m,ρ) and not on q.\n\nThis establishes the asserted pointwise expansion, uniformly over all n and prefixes with |supp(q_n)|≤m. Averaging over prefixes (by linearity of expectation) yields the stated expansion for the per-step expected rejection with the same O-constant. ∎"
  },
  {
    "statement": "(Increasing-order optimality and stepwise characterization of SD) Fix a finite horizon T. Among all unbiased token-level draft–verify algorithms that use exactly one proposal per position with the same proposal conditionals (p_n), standard SD minimizes E[φ(N_{\\mathrm{rej}})] for every function φ that is nondecreasing on {0,1,...,T}. Equivalently, N_{\\mathrm{rej}}^{\\mathrm{SD}} is minimal in the increasing (stochastic) order among all such algorithms. In particular, SD minimizes the mean, the second moment, and all exponential moments of N_{\\mathrm{rej}}; consequently, within any subclass of algorithms having the same mean, it minimizes the variance. Moreover, any algorithm that is optimal for φ(t)=t (and hence any algorithm that is optimal for all such nondecreasing φ) must, at every step and every prefix, use a maximal-agreement coupling between q_n(\\cdot\\mid x) and p_n(\\cdot\\mid x), i.e., it must achieve P\\{X_n=Y_n\\mid x\\}=1-\\operatorname{TV}(q_n(\\cdot\\mid x),p_n(\\cdot\\mid x)).",
    "proof_markdown": "\\begin{proof}\nFix $T\\in\\mathbb N$ and a finite alphabet $\\mathcal V$. Let $q$ be the target AR model with conditionals $q_n(\\cdot\\mid x_{1:n-1})$ and let $p$ denote the draft conditionals $p_n(\\cdot\\mid x_{1:n-1})$. Consider any unbiased token-level draft–verify algorithm $A$ that uses exactly one proposal per position and the given $(p_n)$; unbiasedness means the output law is $q$. At step $n$, after the accepted prefix $X_{1:n-1}=x_{1:n-1}$, the algorithm draws $Y_n\\sim p_n(\\cdot\\mid x_{1:n-1})$ and outputs $X_n\\in\\mathcal V$ with marginal $q_n(\\cdot\\mid x_{1:n-1})$. Define $R_n\\equiv\\mathbf 1\\{X_n\\ne Y_n\\}$ and $N_{\\mathrm{rej}}\\equiv\\sum_{n=1}^T R_n$.\n\nStep 1 (Reduction to couplings at each step). For each $n$ and prefix $x\\equiv x_{1:n-1}$, the pair $(X_n,Y_n)$ under $A$ induces a coupling $K_n^A(\\cdot,\\cdot\\mid x)$ of $q_n(\\cdot\\mid x)$ and $p_n(\\cdot\\mid x)$. Conversely, any family of couplings $K_n(\\cdot,\\cdot\\mid x)$ can be realized by first sampling $Y_n\\sim p_n(\\cdot\\mid x)$ and then $X_n\\sim K_n(\\cdot\\mid Y_n,x)$, which preserves $X_n\\sim q_n(\\cdot\\mid x)$. Since unbiasedness enforces $X_{1:n-1}\\sim q$, expectations at step $n$ are taken w.r.t. $x\\sim q$ for every unbiased algorithm.\n\nStep 2 (Dynamic program). Let $\\varphi$ be nondecreasing on $\\{0,1,\\dots,T\\}$. For $n\\in\\{1,\\dots,T+1\\}$, $x\\in\\mathcal V^{n-1}$ and $s\\in\\{0,1,\\dots,n-1\\}$ define\n$$\nW_n(x,s)\\equiv\\inf_{\\text{valid one-proposal algorithms from step }n}\\ \\mathbb E\\big[\\,\\varphi\\big(s+\\textstyle\\sum_{j=n}^T R_j\\big)\\,\\big|\\,X_{1:n-1}=x\\big].\n$$\nSet $W_{T+1}(x,s)=\\varphi(s)$ for all $x,s$. For $n\\le T$ and any coupling $K$ of $q_n(\\cdot\\mid x)$ and $p_n(\\cdot\\mid x)$ we have\n$$\n\\mathbb E_{(u,v)\\sim K}\\big[\\,W_{n+1}(xu,\\ s+\\mathbf 1\\{u\\ne v\\})\\,\\big]\n= \\mathbb E_{u\\sim q_n(\\cdot\\mid x)}\\big[W_{n+1}(xu,s)\\big]\n+ \\mathbb E_{(u,v)\\sim K}\\big[\\Delta_{n+1}(xu;s)\\,\\mathbf 1\\{u\\ne v\\}\\big],\n$$\nwhere $\\Delta_{n+1}(xu;s)\\equiv W_{n+1}(xu,s+1)-W_{n+1}(xu,s)\\ge 0$, by backward induction from the assumption that $\\varphi$ is nondecreasing on $\\{0,\\dots,T\\}$. Hence, for fixed $(x,s)$, minimizing the Bellman expression is equivalent to maximizing\n$$\n\\sum_{u\\in\\mathcal V} \\Delta_{n+1}(xu;s)\\, K(u,u)\\quad\\text{over all couplings $K$ of $q_n(\\cdot\\mid x)$ and $p_n(\\cdot\\mid x)$}.\n$$\n\nStep 3 (Optimal stepwise coupling via a tight upper bound and explicit attainment). Fix $(x,s)$ and abbreviate $q\\equiv q_n(\\cdot\\mid x)$, $p\\equiv p_n(\\cdot\\mid x)$, and $\\Delta(u)\\equiv \\Delta_{n+1}(xu;s)\\ge 0$. For any coupling $K$ of $(q,p)$, each diagonal entry satisfies\n$$\nK(u,u)\\le \\min\\{q(u),p(u)\\}\\qquad(\\forall u\\in\\mathcal V),\n$$\nso, using nonnegativity of the weights,\n$$\n\\sum_{u}\\Delta(u)\\,K(u,u)\\ \\le\\ \\sum_{u}\\Delta(u)\\,\\min\\{q(u),p(u)\\}.\\tag{*}\n$$\nWe claim the upper bound (*) is attained. Define the overlap and residuals\n$$\n\\rho(u)\\equiv \\min\\{q(u),p(u)\\},\\qquad r(u)\\equiv [\\,q(u)-p(u)\\,]_+,\\qquad s(u)\\equiv [\\,p(u)-q(u)\\,]_+.\n$$\nThen $\\sum_u r(u)=\\sum_u s(u)=\\operatorname{TV}(q,p)$ and $r(u)\\,s(u)=0$ for every $u$. Choose any matrix $L$ supported on $\\{(i,j): r(i)>0,\\ s(j)>0\\}$ with row sums $r(i)$ and column sums $s(j)$. Now define a coupling $K^*$ by\n$$\nK^*(u,u)=\\rho(u)\\quad(\\forall u),\\qquad K^*(i,j)=L(i,j)\\ \\text{ for }\\ i\\in\\{r>0\\},\\ j\\in\\{s>0\\},\n$$\nand $K^*(i,j)=0$ otherwise. Then $K^*$ has marginals $(q,p)$, achieves $K^*(u,u)=\\min\\{q(u),p(u)\\}$ for all $u$, and hence attains equality in (*). Consequently, for the Bellman step at $(x,s)$, any maximal-agreement coupling (one with $K(u,u)=\\min\\{q(u),p(u)\\}$ for all $u$) is optimal. In particular, this coupling agrees with the standard token-level SD rule: with probability $\\sum_u \\min\\{q(u),p(u)\\}=1-\\operatorname{TV}(q,p)$ it sets $X=Y$, and otherwise draws $X$ from the residual $[q-p]_+/\\operatorname{TV}(q,p)$.\n\nBackward induction (sufficiency). Since at every $(x,s)$ a maximal-agreement coupling is optimal for the Bellman step, applying it at each $n=1,\\dots,T$ yields a globally optimal policy. Hence\n$$\n\\mathbb E_{\\mathrm{SD}}\\big[\\varphi(N_{\\mathrm{rej}})\\big]=\\mathbb E\\big[W_1(\\emptyset,0)\\big]\\ \\le\\ \\mathbb E_A\\big[\\varphi(N_{\\mathrm{rej}})\\big]\n$$\nfor every unbiased one-proposal algorithm $A$ with proposal conditionals $(p_n)$. Equivalently, $N_{\\mathrm{rej}}^{\\mathrm{SD}}$ is minimal in the increasing (stochastic) order among all such algorithms.\n\nNecessity of maximal agreement for mean-optimality. Take $\\varphi(t)=t$. Then $\\Delta_{n+1}(xu;s)\\equiv 1$ for all states, so the Bellman step reduces to maximizing $\\sum_u K(u,u)$ subject to $K(u,u)\\le\\min\\{q(u),p(u)\\}$, which forces $K(u,u)=\\min\\{q(u),p(u)\\}$ for every $u$. Thus any algorithm that minimizes $\\mathbb E[N_{\\mathrm{rej}}]$ must, at every step and prefix, use a maximal-agreement coupling; in particular, any algorithm that is simultaneously optimal for all nondecreasing $\\varphi$ coincides with SD on the agreement event and attains $\\mathbb P\\{X_n=Y_n\\mid x\\}=1-\\operatorname{TV}(q_n(\\cdot\\mid x),p_n(\\cdot\\mid x))$ at each step.\n\nConsequences. Because the inequality holds for every nondecreasing $\\varphi$ on $\\{0,\\dots,T\\}$:\n- With $\\varphi(t)=t$, SD minimizes $\\mathbb E[N_{\\mathrm{rej}}]$ and attains the instance-dependent lower bound $\\sum_{n=1}^T\\mathbb E_{x\\sim q}[\\operatorname{TV}(p_n(\\cdot\\mid x),q_n(\\cdot\\mid x))]$.\n- With $\\varphi(t)=t^2$, SD minimizes the second moment $\\mathbb E[N_{\\mathrm{rej}}^2]$. Consequently, within any class of unbiased algorithms having the same mean $\\mathbb E[N_{\\mathrm{rej}}]$, SD minimizes the variance.\n- With $\\varphi(t)=e^{\\lambda t}$ for $\\lambda>0$, SD minimizes all exponential moments of $N_{\\mathrm{rej}}$.\n\nThus, among all unbiased token-level draft–verify algorithms using exactly one proposal per position with the same conditionals $(p_n)$, standard SD minimizes $\\mathbb E[\\varphi(N_{\\mathrm{rej}})]$ for every nondecreasing $\\varphi$ on $\\{0,\\dots,T\\}$, establishing increasing-order optimality and characterizing stepwise optimality via maximal-agreement couplings. $\\square$\n\\end{proof}"
  }
]