(Restart rule that enforces convexity and preserves rate.) Let f be convex and L-smooth. Run GD with any \eta\in(\tfrac{7}{4L},\tfrac{2}{L}) and restart (set x_{n+1}=x_n) whenever \Delta_n>\Delta_{n-1}. Then the piecewise run between restarts has a convex optimization curve, and the overall complexity to reach f_n-f_\star\le \varepsilon matches, up to a universal constant factor, that of the best fixed \eta\in(0,\tfrac{2}{L}] chosen in hindsight.