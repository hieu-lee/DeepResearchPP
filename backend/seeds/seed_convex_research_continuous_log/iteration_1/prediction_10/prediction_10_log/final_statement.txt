Let f be convex and L-smooth. Run gradient descent with any stepsize $\eta\in(\tfrac{7}{4L},\tfrac{2}{L})$ and restart (set $x_{n+1}=x_n$ and reset the comparison threshold) whenever $\Delta_n>\Delta_{n-1}$, where $\Delta_n:=f(x_n)-f(x_n-\eta\nabla f(x_n))$. Then, between restarts, the sequence $n\mapsto f(x_n)$ is convex. Moreover, there is no universal constant $C$ such that, for all convex $L$-smooth $f$ and all $\varepsilon>0$, the number of iterations required by this restarted scheme to achieve $f(x_n)-f_\star\le\varepsilon$ is at most $C$ times that of the best fixed stepsize $\eta\in(0,\tfrac{2}{L}]$ chosen in hindsight.