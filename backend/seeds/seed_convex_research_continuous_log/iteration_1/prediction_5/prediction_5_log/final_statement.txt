Non-monotonicity of per-iteration decrease ratios. There exist 0<\mu<L, a stepsize \eta\in(0,2/L), a \mu-strongly convex and L-smooth function f, and an initialization x_0 such that for the gradient descent iterates x_{n+1}=x_n-\eta\nabla f(x_n), with r_n:=\tfrac{f(x_{n+1})-f_\star}{f(x_n)-f_\star} and q:=\max_{\lambda\in[\mu,L]}|1-\eta\lambda|^2, the sequence \{r_n\} is strictly increasing (hence not nonincreasing). Moreover, in this example \lim_{n\to\infty} r_n=q.