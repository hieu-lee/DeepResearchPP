(One-step lookahead step-size ensuring convexity of the optimization curve.) For convex L-smooth f, define at step n the maximal step-size \hat\eta_n:=\sup\{\eta\in(0,\tfrac{2}{L}]: f(x_n)-f(x_{n+1}(\eta))\ge f(x_{n-1})-f(x_n)\}, where x_{n+1}(\eta)=x_n-\eta g_n. Then GD with \eta_n\equiv \hat\eta_n produces a convex optimization curve and achieves the same worst-case rate as GD with the best fixed \eta\in(0,\tfrac{2}{L}] chosen in hindsight.