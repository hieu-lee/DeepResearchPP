[
  "Let f be convex and L-smooth. Gradient descent with step size η ∈ (0, 1.75⁄L] induces a convex optimization curve: the mapping n ↦ f(xₙ) is convex (equivalently, the sequence f(xₙ) − f(xₙ₊₁) is non-increasing).",
  "For any L > 0 there exists a convex L-smooth function and initialization such that for every step-size η ∈ (1.75⁄L, 2⁄L), the gradient descent optimization curve is not convex (despite converging and monotonically decreasing).",
  "For convex L-smooth f and η ∈ (0, 2⁄L], the sequence of gradient norms {‖∇f(xₙ)‖} is non-increasing.",
  "For convex L-smooth f, the gradient flow optimization curve t ↦ f(x(t)) is always convex.",
  "For convex L-smooth f, under gradient flow, the function t ↦ ‖∇f(x(t))‖ is non-increasing.",
  "Fix any L>0 and any relative noise level \\(\\delta\\in(0,1)\\). There does not exist a positive universal step size \\(\\eta_{\\max}(\\delta,L)>0\\) with the property that for every (even one‑dimensional) convex L‑smooth quadratic f, every nonzero initialization x_0\\ne0, and every sequence of inexact‑gradient noises satisfying \\(\\|e_n\\|\\le\\delta\\|\\nabla f(x_n)\\|\\), the inexact gradient descent iterates \\(x_{n+1}=x_n-\\eta(\\nabla f(x_n)+e_n)\\) produce a convex sequence of function values \\(n\\mapsto f(x_n)\\) for every choice of step size \\(0<\\eta\\le\\eta_{\\max}(\\delta,L)\\). In particular, no nonzero universal convexity‑preserving step‑size depending only on \\(\\delta\\) and \\(L\\) (and valid for all one‑dimensional convex L‑smooth quadratics and all nonzero initializations) exists when \\(\\delta>0\\).",
  "Suppose f: \\mathbb{R}^d\\to\\mathbb{R} is convex and twice continuously differentiable on the sublevel set S:=\\{x:\\;f(x)\\le f(x_0)\\} for some initial point x_0. Assume there exist a scalar \\kappa>0 and a symmetric positive semidefinite matrix A with largest eigenvalue L_A such that for every x\\in S one has\n\\[\\nabla^2 f(x)\\preceq \\kappa A.\\]\nSet L_{\\mathrm{eff}}:=\\kappa L_A. Then gradient descent with constant step size\n\\[\\n\\eta\\in\\bigl(0,\\,1.75/ L_{\\mathrm{eff}}\\bigr]\\]\nstarted at x_0 produces a convex optimization curve n\\mapsto f(x_n); equivalently the forward differences \\Delta_n:=f(x_n)-f(x_{n+1}) form a nonincreasing sequence. In particular, if \\kappa=1 and L_A=L then convexity holds for every \\eta\\in(0,\\,1.75/L].",
]