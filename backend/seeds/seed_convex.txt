[
    "The mapping $n\\mapsto f(x_n)$ induced by gradient descent on a convex $L$-smooth function is convex (equivalently, the sequence $\\{f(x_n)-f(x_{n+1})\\}_{n=0}^{\\infty}$ is non-increasing) whenever the constant step size satisfies $\\eta\\in(0,1.75/L]$.",
    "There exists a convex $L$-smooth function and initial point such that for every step size $\\eta\\in(1.75/L,2/L)$, the optimization curve is not convex — even though it still decreases monotonically.",
    "For convex $L$-smooth functions and gradient descent with step size $\\eta\\in(0,2/L]$, the sequence of gradient norms $\\{\\|\\nabla f(x_n)\\|\\}_{n=0}^{\\infty}$ is non-increasing.",
    "For convex $L$-smooth functions, the optimization curve induced by gradient flow is always convex.",
    "Moreover, under gradient flow, the mapping $t\\mapsto \\|\\nabla f(x(t))\\|$ is non-increasing over $[0,\\infty)$.",
    "Let f(x)=\\tfrac12 x^\\top A x + b^\\top x + c with A\\succeq 0 and \\|A\\|\\le L. Consider gradient descent x_{n+1}=x_n-\\eta(Ax_n+b) with step size \\eta\\in(0,2/L), and let x_* be any minimizer of f. Define d_n:=\\|x_n-x_*\\|^2. Then for all m\\ge1 and n\\ge0, (-1)^m\\,\\Delta^m d_n\\ge0, where \\Delta d_n:=d_{n+1}-d_n and \\Delta^m:=\\Delta\\circ\\cdots\\circ\\Delta (m times). In particular, {d_n} is decreasing and convex.",
    "(Bounded total negative discrete curvature) If f is L-smooth and GD uses any step $\\eta\\in(0,2/L]$, then the total negative curvature of the discrete optimization curve is uniformly bounded by the initial suboptimality: there exists a universal constant $C>0$ such that $\\sum_{n=0}^{\\infty} [-(g_{n+2}-2g_{n+1}+g_n)]_+ \\le C\\,(f(x_0)-f_*)$.",
    "There exist L>0, a convex L-smooth function f, a point x_n, a stepsize η∈(0,2/L], and s∈[0,η] such that, with y(s) the gradient flow from x_n (i.e., ȳ(t)=-∇f(y(t)), y(0)=x_n) and x_{n+1}=x_n-η∇f(x_n) the one-step gradient-descent iterate, one has\n$$\nf(y(s))>(1-\\tfrac{s}{\\eta})\\,f(x_n)+ (\\tfrac{s}{\\eta})\\,f(x_{n+1}).\n$$",
    "Global GF–GD majorization is false: there exist a convex L-smooth function f, a step size η∈(0,2/L], an initial point x0, and a time t≥0 such that, if x^{GF} solves gradient flow with x^{GF}(0)=x0 and \\tilde g denotes the piecewise-linear interpolant through the gradient-descent samples {(nη, g_n)} generated from x0 with step size η, then f(x^{GF}(t))>\\tilde g(t).",
    "There exists τ∈(0,2), a convex L-smooth function f, and an initial point x_0≠0 such that for gradient descent with step size η=τ/L, no choice of constants 0<θ_-≤1≤θ_+<∞ can make the inequalities f(x^{\\mathrm{GF}}(θ_- nη)) ≤ g_n ≤ f(x^{\\mathrm{GF}}(θ_+ nη)) hold for all n≥0 (where g_n:=f(x_n)).",
    "statement": "If f: \\mathbb{R}^d\\to\\mathbb{R} is convex, L-smooth, and attains its minimum value f_* (i.e., \\operatorname{argmin} f \\neq \\emptyset), and x(\\cdot) solves the gradient flow \\(\\dot x(t)=-\\nabla f(x(t))\\) with x(0)=x_0, then t\\mapsto t\\,(f(x(t)) - f_*) is non-increasing on [1/L,\\infty)."
]
