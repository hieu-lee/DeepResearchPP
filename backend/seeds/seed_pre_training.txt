[
  "$$\n\textbf{(Chinchilla Compute-Optimal Scaling)} \quad\n\hat{L}(N,D) \;=\; E \;+\; \frac{A}{N^{\alpha}} \;+\; \frac{B}{D^{\beta}}\n\quad \text{subject to} \quad\nC \;\approx\; 6ND.\n$$\n\n$$\n\min_{N,D} \ \hat{L}(N,D) \quad \Longrightarrow \quad\nN_\star(C) \;\propto\; C^{0.49}, \n\qquad\nD_\star(C) \;\propto\; C^{0.51}.\n$$\n\n$$\n\text{Interpretation: For compute budget } C, \ \nN \text{ (parameters) and } D \text{ (tokens) should grow equally.}\n$$",
  "Scaling with context length under general compute inflation: Let A,B,α,β>0 and consider the loss model \(\hat L(N,D)=\frac{A}{N^{\alpha}}+\frac{B}{D^{\beta}}\) optimized over \(N,D>0\) under the compute constraint \(C=\kappa(L)\,N D\) with \(C>0\) and \(\kappa(L)>0\) for all \(L\ge 0\). Then the compute-optimal \(C\)-exponents of \(N_\star\) and \(D_\star\) are independent of the \(L\)-dependence in \(\kappa(L)\). Moreover, writing \(g(L):=\kappa(L)/\kappa(0)\) and defining the context-aware tokens-per-parameter by \n\n\n\(\rho^{\mathrm{ca}}:= (D/N)\,g(L)^{-\beta/(\alpha+\beta)},\)\n\n\nwe have, at fixed \(C\),\n\n\n\(\rho^{\mathrm{ca}}_\star(C,L)=\rho^{\mathrm{ca}}_\star(C,0)\,g(L)^{-\alpha/(\alpha+\beta)}.\)\n\nIf a mechanism replaces \(\kappa\) by \(\kappa_\sigma\) with \(\kappa_\sigma(0)=\kappa(0)\) and \(\kappa_\sigma(L)<\kappa(L)\) for \(L>0\), then for the same \(C\) and any \(L>0\) one has strictly larger optimal \(N_\star\) and \(D_\star\) and strictly lower optimal loss \(\hat L_\star\). In particular, taking \(\kappa(L)=\kappa_0(1+\chi L^p)\) and \(\kappa_\sigma(L)=\kappa_0(1+\sigma\chi L^p)\) with \(\kappa_0>0\), \(\chi,p>0\), and \(0<\sigma<1\) satisfies these conditions.",
  "Duplication-robust compute allocation (tight). Let A,B,\alpha,\beta,M,r>0. Consider\n$$\min_{N,D>0}\; A N^{-\alpha}+B\,(D/r)^{-\beta}\quad\text{subject to}\quad ND=M.$$\nThen there is a unique global minimizer given exactly by\n$$N_*(M,r)=\Big(\tfrac{\alpha A}{\beta B}\Big)^{\!\tfrac1{\alpha+\beta}} M^{\tfrac{\beta}{\alpha+\beta}} r^{-\tfrac{\beta}{\alpha+\beta}},\qquad D_*(M,r)=\Big(\tfrac{\beta B}{\alpha A}\Big)^{\!\tfrac1{\alpha+\beta}} M^{\tfrac{\alpha}{\alpha+\beta}} r^{\tfrac{\beta}{\alpha+\beta}}.$$\nConsequently,\n$$\rho_*(M,r)=\frac{D_*}{N_*}=\Big(\tfrac{\beta B}{\alpha A}\Big)^{\!\tfrac{2}{\alpha+\beta}} M^{\tfrac{\alpha-\beta}{\alpha+\beta}} r^{\tfrac{2\beta}{\alpha+\beta}}=r^{\tfrac{2\beta}{\alpha+\beta}}\,\rho_*(M,1).$$\nSince $M$ is proportional to total compute $C$ (e.g., if $C=\kappa ND$ with fixed $\kappa>0$, then $M=C/\kappa$), this yields the compute scalings $N_*\propto C^{\beta/(\alpha+\beta)} r^{-\beta/(\alpha+\beta)}$, $D_*\propto C^{\alpha/(\alpha+\beta)} r^{\beta/(\alpha+\beta)}$, and $\rho_*\propto r^{2\beta/(\alpha+\beta)}\,\rho_*(C,1)$. In particular, when $\alpha=\beta$ one has $N_*\propto C^{1/2}r^{-1/2}$, $D_*\propto C^{1/2}r^{1/2}$, and $\rho_*\propto r$.",
  "Inference-aware breakpoint for model shrinkage (tightened): Under the additive budget $\kappa N D+\varphi N Q\le B_{\mathrm{tot}}$ with $\kappa,\varphi,B_{\mathrm{tot}}>0$, the square-root shrinkage $N_{\mathrm{opt}}(B_{\mathrm{tot}},Q)\asymp N_\star(B_{\mathrm{tot}},0)\,(Q/Q_c)^{-1/2}$ and $D_{\mathrm{opt}}(B_{\mathrm{tot}},Q)\asymp D_\star(B_{\mathrm{tot}},0)\,(Q/Q_c)^{1/2}$ is infeasible for all sufficiently large $Q$ (this conclusion is independent of the loss). Moreover, if the loss is $L(N,D)=E+A N^{-\alpha}+B D^{-\beta}$ with $A,B>0$ and $\alpha,\beta>0$, then for each $Q\ge0$ the optimizer $(N_{\mathrm{opt}},D_{\mathrm{opt}})$ is unique; there is a breakpoint $Q_c=(\kappa/\varphi)\,D_\star(B_{\mathrm{tot}},0)$ such that, as $Q\to\infty$, $$N_{\mathrm{opt}}(B_{\mathrm{tot}},Q)\sim N_\star(B_{\mathrm{tot}},0)\Big(\frac{Q}{Q_c}\Big)^{-1},\qquad D_{\mathrm{opt}}(B_{\mathrm{tot}},Q)\sim D_\star(B_{\mathrm{tot}},0)\Big(\frac{Q}{Q_c}\Big)^{\frac{1-\alpha}{\beta+1}}.$$",
  "Inference-aware exponents with additive budget (tightened). Fix A,B,α,β,κ>0 and φ≥0. Under the budget constraint κ N D + φ N Q ≤ B_tot for parameters N, training tokens D, and inference tokens Q≥0, the compute-optimal allocations satisfy\n\nD_opt(B_tot,Q) = K_D B_tot^{α/(α+β)} s^{(α−1)/(α+β)},\nN_opt(B_tot,Q) = K_N B_tot^{β/(α+β)} s^{(β+1)/(α+β)},\n\nwhere s∈(0,1] is the unique solution to\n\n1/s = 1 + Λ (Q/B_tot^{α/(α+β)}) s^{(1−α)/(α+β)},\n\nand the constants are\n\nK_D = ((β B)/(α A))^{1/(α+β)} κ^{−α/(α+β)},\nK_N = ((α A)/(β B))^{1/(α+β)} κ^{−β/(α+β)},\nΛ = φ ((α A)/(β B))^{1/(α+β)} κ^{−β/(α+β)} ≥ 0.\n\nAt fixed B_tot, if φ>0 then s is strictly decreasing in Q, hence N_opt is strictly decreasing in Q, and the tokens-per-parameter ratio ρ_opt = D_opt/N_opt is strictly increasing in Q if α−β−2<0, strictly decreasing if α−β−2>0, and constant if α−β−2=0. If φ=0 (or Q=0) then s=1 and N_opt, D_opt are independent of Q, reducing to the standard Chinchilla scalings.\n\nMoreover, if φ>0 then as Q→∞ with fixed B_tot, s ∼ (Λ Q / B_tot^{α/(α+β)})^{−(α+β)/(β+1)} and N_opt(B_tot,Q) ∼ B_tot/(φ Q). In particular, there is no λ>0 for which N_opt(B_tot,Q) ≍ B_tot^{β/(α+β)} (1 + λ Q / B_tot)^{−β/(α+β)} holds uniformly in Q (this remains true a fortiori when φ=0, since N_opt is then independent of Q while the template decays in Q).",
  "Quality-adaptive sampling in two pools. Under the compute-constrained model C = κ N(D_1 + D_2) and loss A N^{-α} + ∑_{i=1}^2 B_i (D_i/r_i)^{-β_i} with α>0 and B_i, β_i, r_i, κ > 0, the compute-optimal tokens-per-parameter ρ⋆ = D⋆/N⋆ does not admit a product-form dependence on the quality multipliers r_i. In particular, in the single-pool case ρ⋆ ∝ r^{2β/(α+β)}, and in the two-pool case ρ⋆ ≍ max_i r_i^{2β_i/(α+β_i)} up to universal constants independent of r. Moreover, if β_1 > β_2, then as C increases the optimal share w_2 strictly increases and satisfies w_2 → 1 as C → ∞; consequently β_eff(C) ↓ β_2 and, if α > β_2, ρ⋆(C) is eventually strictly increasing in C.",
  "Embedding-parameter correction under a compute constraint (embeddings excluded from compute). Let L(N_{tot},D)=A/N_{tot}^{\alpha}+B/D^{\beta} with A,B,\alpha,\beta>0 and compute constraint \kappa N D=C, where embeddings of size N_e do not enter compute, so N_{tot}=N+N_e. Let N_0 be the embedding-free optimizer (N_e=0) and set \rho_{\star}^{(0)}:=C/(\kappa N_0^2). Then:\n\n(i) Interior optimum and exact correction. Whenever the global minimizer is interior, the optimizer (N_\star,D_\star) satisfies\n\[(1+N_e/N_\star)^{\alpha+1}=(N_0/N_\star)^{\alpha+\beta},\]\nwhence\n\[\rho_{\mathrm{tot},\star}=\rho_{\star}^{(0)}\,(1+N_e/N_\star)^{\frac{2(\alpha+1)}{\alpha+\beta}-1}.\]\nMoreover, if \beta>1 the global minimizer exists uniquely and is interior, so the preceding identities always hold in that case.\n\n(ii) Boundary optimum. If the global minimizer is the boundary N\to 0 (which can occur only when \beta\le 1 and N_e>0), then \rho_{\mathrm{tot},\star}=\infty. For \beta=1 one has the explicit threshold: the minimizer is interior iff\n\[C>\frac{B\,\kappa}{\alpha A}\,N_e^{\alpha+1},\]\notherwise N\to 0 is optimal and \rho_{\mathrm{tot},\star}=\infty.\n\n(iii) Small-embedding asymptotics. For small N_e one has\n\[N_\star=N_0-\frac{\alpha+1}{\alpha+\beta}\,N_e+o(N_e),\]\nso ignoring embeddings overstates the optimal core size by \frac{\alpha+1}{\alpha+\beta}N_e+o(N_e).\n\nIn particular, the putative correction \(\rho_{\mathrm{tot},\star}=\rho_{\star}^{(0)}(1+N_e/N_\star)^{-1}\) is false (except trivially when N_e=0), and the proposed shrink relation \(N_\star=N_0\,(1+N_e/N_\star)^{-\beta/(\alpha+\beta)}\) is false in general, holding only in the nongeneric case \(\alpha+1=\beta\) (or when N_e=0)."
]