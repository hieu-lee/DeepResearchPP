[
    "
    Consider a Dense Associative Memory (modern Hopfield network) storing $P$ random patterns $\{\xi^\mu\}_{\mu=1}^P \subset \mathbb{R}^N$ drawn i.i.d.\ from a distribution with mean-squared norm $\mathbb{E}\|\xi\|^2 = N$.  
    Define the load parameter as
    $$P = \exp(\alpha N).$$
    There exist two critical thresholds in the asymptotic limit $N \to \infty$:
    1. The \emph{typical-pattern retrieval threshold}, $\alpha_1(\lambda)$: if $\alpha < \alpha_1(\lambda)$, a randomly selected pattern can be retrieved with high probability.
    2. The (rigorous) lower bound for the \emph{all-patterns retrieval threshold}, $\alpha_c(\lambda)$: if $\alpha < \alpha_c(\lambda)$, all $P$ patterns can be retrieved (with high probability).
    Here $\lambda$ is the inverse-temperature parameter appearing in the network's energy function
    $$E(x) = -\frac{1}{\lambda}\log\sum_{\mu=1}^P \exp(\lambda\, x\cdot \xi^\mu) + \frac12\|x\|^2.$$
    Thus the network achieves \emph{exponential memory capacity} $P = \exp(\alpha N)$ in $N$, with distinct asymptotic thresholds for typical vs.\ worst-case (all-pattern) retrieval.
    "
]