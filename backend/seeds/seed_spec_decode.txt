[
  "\textbf{(Exactness)}\quad \forall\, x_{1:T}\in\mathcal{V}^T:\; P_{\mathrm{SD}}(x_{1:T}) = q(x_{1:T})\ \text{(speculative decoding matches the target model's joint distribution).}",
  "\textbf{(Expected rejections \& speedup)}\quad \mathbb{E}[N_{\mathrm{rej}}] \;=\; \sum_{n=1}^T \mathbb{E}_{x_{1:n-1}\sim q}\Big[\operatorname{TV}\!\big(p_n(\cdot\mid x_{1:n-1}),\, q_n(\cdot\mid x_{1:n-1})\big)\Big],\ \ \text{and}\ \ \mathrm{SpeedUp}=\dfrac{T}{\mathbb{E}[N_{\mathrm{rej}}]}. \ \text{(In the i.i.d.\ case, }\mathrm{SpeedUp}=\frac{1}{\mathbb{E}[\operatorname{TV}(p,q)]}\text{).}",
  "\textbf{(Optimality among unbiased methods)}\quad \inf_{A\in\mathcal{F}_{\mathrm{unbiased}}}\ \mathbb{E}[N_{\mathrm{rej}}^{A}] \;\ge\; \sum_{n=1}^T \mathbb{E}_{x_{1:n-1}\sim q}\!\left[\operatorname{TV}\!\big(p_n(\cdot\mid x_{1:n-1}),\, q_n(\cdot\mid x_{1:n-1})\big)\right],\ \text{so standard SD attains the instance-dependent lower bound.}",
  "\textbf{(Batch SD: unbiasedness and diminishing returns)}\quad P_{\mathrm{Batch}}(x_{1:T})=q(x_{1:T})\ \text{for all }x_{1:T}.\ \text{Moreover, letting } q^{(1)}_n=q_n,\ q^{(m+1)}_n=[\,q^{(m)}_n - p_n\,]_+,\ \text{the expected rejections }\mathbb{E}[N_{\mathrm{rej}}^{(M)}]\ \text{are an explicit function of }(p_n,q_n)_{n\le T},\ \text{nonincreasing in }M,\ \text{and } \lim_{M\to\infty}\mathbb{E}[N_{\mathrm{rej}}^{(M)}] > 0\ \text{unless }p_n\equiv q_n.",
  "\textbf{(Block verification optimality)}\quad \text{For fixed draft length }K,\ \text{block-level verification maximizes the expected accepted-token count per iteration among all valid draft-verification algorithms and is never worse than token-level verification.}",
  "\textbf{(Info-theoretic speed limits)}\quad \text{With }m\text{ drafted tokens, the achievable speedup is governed by the entropy of the acceptance distribution }H(A_m)\ \text{and admits bounds in terms of }D_{\mathrm{KL}}(q\Vert p)\ \text{(via connections to channel simulation / Tunstall coding).}",
  "\textbf{(OT-based drafting guarantee)}\quad \text{With }k\text{ candidate tokens per position, the OT-based sequential selection achieves acceptance probability at least }(1-\tfrac{1}{e})\text{ times the optimal plan, while preserving exactness.}",
  "Fix block size K and proposal conditionals (p_{n:n+K-1}). Among all unbiased K-block draft-verify schemes, the longest-prefix verifier first-order stochastically dominates any other valid scheme: for all j\in\{0,\dots,K\}, \mathbb{P}_{\mathrm{LP}}[A^{(K)}\ge j] \ge \mathbb{P}_{\text{any valid}}[A^{(K)}\ge j]. In particular, it maximizes \mathbb{E}[A^{(K)}].",
  "Local temperature-sensitivity law (pointwise, uniform over bounded support). Fix m\in\mathbb{N} and any \rho\in(0,1). For each step n and prefix x_{1:n-1}, let q_n(\cdot|x_{1:n-1}) be a distribution supported on a finite set S(n,x_{1:n-1}) with |S(n,x_{1:n-1})|\lem. For |\tau-1|\le\rho, define the temperature-scaled proposal on this support by\n$$p_{\tau,n}(v)=\frac{q_n(v)^{\tau}}{\sum_{u\in S(n,x_{1:n-1})} q_n(u)^{\tau}}\quad(v\in S(n,x_{1:n-1})),\qquad p_{\tau,n}(v)=0\ (v\notin S(n,x_{1:n-1})).$$\nThen, uniformly over all n and prefixes,\n$$\operatorname{TV}(p_{\tau,n},q_n)=\tfrac{|\tau-1|}{2}\,\mathbb{E}_{q_n}\big[\,|\log q_n(V)-\mathbb{E}_{q_n}[\log q_n(V)]|\,\big]+O\big((\tau-1)^2\big),$$\nwhere the O-constant depends only on m and \rho (and not on n, the prefix, or q_n). Consequently, averaging over prefixes yields the per-step expected rejection\n$$\tau_n(\tau)=\mathbb{E}_{x_{1:n-1}}\big[\operatorname{TV}(p_{\tau,n},q_n)\big]=\tfrac{|\tau-1|}{2}\,\mathbb{E}_{x_{1:n-1}}\Big[\mathbb{E}_{q_n}\big[\,|\log q_n(V)-\mathbb{E}_{q_n}[\log q_n(V)]|\,\big]\Big]+O\big((\tau-1)^2\big),$$\nwith the same O-constant.",
  "(Increasing-order optimality and stepwise characterization of SD) Fix a finite horizon T. Among all unbiased token-level draft-verify algorithms that use exactly one proposal per position with the same proposal conditionals (p_n), standard SD minimizes E[\phi(N_{\mathrm{rej}})] for every function \phi that is nondecreasing on {0,1,...,T}. Equivalently, N_{\mathrm{rej}}^{\mathrm{SD}} is minimal in the increasing (stochastic) order among all such algorithms. In particular, SD minimizes the mean, the second moment, and all exponential moments of N_{\mathrm{rej}}; consequently, within any subclass of algorithms having the same mean, it minimizes the variance. Moreover, any algorithm that is optimal for \phi(t)=t (and hence any algorithm that is optimal for all such nondecreasing \phi) must, at every step and every prefix, use a maximal-agreement coupling between q_n(\cdot\mid x) and p_n(\cdot\mid x), i.e., it must achieve P\{X_n=Y_n\mid x\}=1-\operatorname{TV}(q_n(\cdot\mid x),p_n(\cdot\mid x))."
]
