[
  "Transformer universality with positional encodings: For any compact set \(K\) of sequences and any continuous sequence-to-sequence map \(F\) on \(K\), there exists a Transformer with positional encodings that uniformly approximates \(F\) on \(K\).",
  "Single-layer self-attention universality (with suitable encodings): A model consisting of one self-attention layer plus a position-wise MLP, equipped with suitable positional encodings and sufficient width, is a universal approximator of continuous sequence-to-sequence maps on compact domains.",
  "Universality with universal relative positional encodings (URPE): Transformers endowed with universal relative positional encodings are universal function approximators on compact sequence domains.",
  "Masked language modeling equals pseudolikelihood: Maximizing the masked-token objective \(\sum_{i=1}^n \log p_\theta(x_i\mid x_{\setminus i})\) is equivalent to maximizing the Markov random field pseudolikelihood of the discrete sequence model.",
  "Pseudolikelihood consistency: Under standard regularity assumptions (e.g., fully visible Boltzmann machines), the pseudolikelihood estimator \(\hat\theta_{\mathrm{PL}}\) is consistent for the true parameter \(\theta^\star\).",
  "Denoising autoencoders \(\leftrightarrow\) score matching: In the small-noise limit, training a denoising autoencoder is equivalent to score matching for the data density; thus denoising learns the data score \(\nabla_x \log p(x)\).",
  "Reverse-complement (RC) equivariant layer characterization: All linear maps \(L\) on DNA sequence representations that commute with the RC group action are characterized explicitly; stacking such RC-equivariant linear maps with compatible nonlinearities yields universal approximation within the RC-equivariant function class.",
  "Invariance and geometric stability reduce sample complexity: For targets invariant (or stable) under a group/action, generalization rates depend on the effective low-frequency/invariant components of the representation rather than the ambient dimension, yielding improved sample complexity bounds.",
  "Group-equivariant neural networks: A representation is equivariant to a group \(G\) if and only if it can be expressed (up to isomorphism) as a group convolution/intertwiner; parameter tying implied by \(G\)-equivariance reduces the hypothesis class without loss of expressivity within the equivariant class.",
  "Transformers as powerful graph learners: With suitable token/structural encodings, plain Transformers match or exceed the expressivity of Weisfeilerâ€“Leman (WL)-bounded message-passing GNNs on graphs.",
  "Structural encodings for graph Transformers: Adding shortest-path, relative-position, or subgraph encodings strictly increases the expressivity of graph Transformers beyond WL-limited MPNNs under stated conditions.",
  "Universal approximation under constraints: Transformers can approximate target functions while exactly satisfying a prescribed family of convex (and certain manifold) constraints, so expressivity is retained under constraint enforcement.",
  "PAC-Bayes generalization bound (canonical form): For any posterior \(Q\) over hypotheses and prior \(P\), with probability at least \(1-\delta\), \(\mathbb{E}[L(f)] \le \widehat L(f) + \sqrt{\tfrac{\mathrm{KL}(Q\|P)+\ln(1/\delta)}{2n}}\); numerous tightened variants hold under additional assumptions.",
  "MLM as discrete denoising: The masked language modeling objective equals the negative log-likelihood of reconstructing masked tokens from their unmasked context under a discrete corruption channel; i.e., it is a denoising objective on sequences.",
  "Equivariance reduces complexity (RC case): For a group \(G\) acting on inputs, the \(G\)-invariant subclass \(H^G=\{h\in H: h(g\cdot x)=h(x)\ \forall g\in G\}\) satisfies empirical Rademacher complexity \(\mathfrak{R}_n(H^G)\le \mathfrak{R}_n(H)\); hence enforcing RC-equivariance (via parameter tying) cannot increase \(\mathfrak{R}_n\) and thus improves sample-efficiency bounds."
]
